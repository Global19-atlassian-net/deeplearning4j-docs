<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="de">
<head>
<!-- Generated by javadoc (1.8.0_151) on Sat May 16 12:24:21 CEST 2020 -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>NDLoss (deeplearning4j 1.0.0-beta7 API)</title>
<meta name="date" content="2020-05-16">
<link rel="stylesheet" type="text/css" href="../../../../../stylesheet.css" title="Style">
<script type="text/javascript" src="../../../../../script.js"></script>
</head>
<body>
<script type="text/javascript"><!--
    try {
        if (location.href.indexOf('is-external=true') == -1) {
            parent.document.title="NDLoss (deeplearning4j 1.0.0-beta7 API)";
        }
    }
    catch(err) {
    }
//-->
var methods = {"i0":10,"i1":10,"i2":10,"i3":10,"i4":10,"i5":10,"i6":10,"i7":10,"i8":10,"i9":10,"i10":10,"i11":10,"i12":10,"i13":10,"i14":10,"i15":10,"i16":10,"i17":10,"i18":10,"i19":10,"i20":10,"i21":10,"i22":10};
var tabs = {65535:["t0","All Methods"],2:["t2","Instance Methods"],8:["t4","Concrete Methods"]};
var altColor = "altColor";
var rowColor = "rowColor";
var tableTab = "tableTab";
var activeTableTab = "activeTableTab";
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar.top">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.top" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.top.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="class-use/NDLoss.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../../index-all.html">Index</a></li>
<li><a href="../../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../../org/nd4j/linalg/factory/ops/NDLinalg.html" title="class in org.nd4j.linalg.factory.ops"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../../../../org/nd4j/linalg/factory/ops/NDMath.html" title="class in org.nd4j.linalg.factory.ops"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../../index.html?org/nd4j/linalg/factory/ops/NDLoss.html" target="_top">Frames</a></li>
<li><a href="NDLoss.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.nd4j.linalg.factory.ops</div>
<h2 title="Class NDLoss" class="title">Class NDLoss</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li><a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">java.lang.Object</a></li>
<li>
<ul class="inheritance">
<li>org.nd4j.linalg.factory.ops.NDLoss</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<hr>
<br>
<pre>public class <span class="typeNameLabel">NDLoss</span>
extends <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">Object</a></pre>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor.summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="memberSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#NDLoss--">NDLoss</a></span>()</code>&nbsp;</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method.summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="memberSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span id="t0" class="activeTableTab"><span>All Methods</span><span class="tabEnd">&nbsp;</span></span><span id="t2" class="tableTab"><span><a href="javascript:show(2);">Instance Methods</a></span><span class="tabEnd">&nbsp;</span></span><span id="t4" class="tableTab"><span><a href="javascript:show(8);">Concrete Methods</a></span><span class="tabEnd">&nbsp;</span></span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr id="i0" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#absoluteDifference-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">absoluteDifference</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                  <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                  <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</code>
<div class="block">Absolute difference loss: <code>sum_i abs( label[i] - predictions[i] )</code><br></div>
</td>
</tr>
<tr id="i1" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#absoluteDifference-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-">absoluteDifference</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                  <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                  <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                  <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce)</code>
<div class="block">Absolute difference loss: <code>sum_i abs( label[i] - predictions[i] )</code><br></div>
</td>
</tr>
<tr id="i2" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#cosineDistance-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-int-">cosineDistance</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
              <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
              <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
              int&nbsp;dimension)</code>
<div class="block">Cosine distance loss: <code>1 - cosineSimilarity(x,y)</code> or <code>1 - sum_i label[i] * prediction[i]</code>, which is<br>
 equivalent to cosine distance when both the predictions and labels are normalized.</div>
</td>
</tr>
<tr id="i3" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#cosineDistance-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-int-">cosineDistance</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
              <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
              <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
              <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
              int&nbsp;dimension)</code>
<div class="block">Cosine distance loss: <code>1 - cosineSimilarity(x,y)</code> or <code>1 - sum_i label[i] * prediction[i]</code>, which is<br>
 equivalent to cosine distance when both the predictions and labels are normalized.</div>
</td>
</tr>
<tr id="i4" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#hingeLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">hingeLoss</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</code>
<div class="block">Hinge loss: a loss function used for training classifiers.</div>
</td>
</tr>
<tr id="i5" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#hingeLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-">hingeLoss</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
         <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce)</code>
<div class="block">Hinge loss: a loss function used for training classifiers.</div>
</td>
</tr>
<tr id="i6" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#huberLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-double-">huberLoss</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
         double&nbsp;delta)</code>
<div class="block">Huber loss function, used for robust regression.</div>
</td>
</tr>
<tr id="i7" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#huberLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-double-">huberLoss</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
         <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
         double&nbsp;delta)</code>
<div class="block">Huber loss function, used for robust regression.</div>
</td>
</tr>
<tr id="i8" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#l2Loss-org.nd4j.linalg.api.ndarray.INDArray-">l2Loss</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;var)</code>
<div class="block">L2 loss: 1/2 * sum(x^2)<br></div>
</td>
</tr>
<tr id="i9" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#logLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">logLoss</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
       <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions)</code>
<div class="block">Log loss, i.e., binary cross entropy loss, usually used for binary multi-label classification.</div>
</td>
</tr>
<tr id="i10" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#logLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-double-">logLoss</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
       <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
       <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
       <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
       double&nbsp;epsilon)</code>
<div class="block">Log loss, i.e., binary cross entropy loss, usually used for binary multi-label classification.</div>
</td>
</tr>
<tr id="i11" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#logPoisson-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-">logPoisson</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
          boolean&nbsp;full)</code>
<div class="block">Log poisson loss: a loss function used for training classifiers.</div>
</td>
</tr>
<tr id="i12" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#logPoisson-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-boolean-">logPoisson</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
          <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
          boolean&nbsp;full)</code>
<div class="block">Log poisson loss: a loss function used for training classifiers.</div>
</td>
</tr>
<tr id="i13" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#meanPairwiseSquaredError-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">meanPairwiseSquaredError</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                        <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                        <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</code>
<div class="block">Mean pairwise squared error.</div>
</td>
</tr>
<tr id="i14" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#meanPairwiseSquaredError-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-">meanPairwiseSquaredError</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                        <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                        <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                        <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce)</code>
<div class="block">Mean pairwise squared error.</div>
</td>
</tr>
<tr id="i15" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#meanSquaredError-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">meanSquaredError</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</code>
<div class="block">Mean squared error loss function.</div>
</td>
</tr>
<tr id="i16" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#meanSquaredError-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-">meanSquaredError</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce)</code>
<div class="block">Mean squared error loss function.</div>
</td>
</tr>
<tr id="i17" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#sigmoidCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">sigmoidCrossEntropy</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictionLogits,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</code>
<div class="block">Sigmoid cross entropy: applies the sigmoid activation function on the input logits (input "pre-sigmoid preductions")<br>
 and implements the binary cross entropy loss function.</div>
</td>
</tr>
<tr id="i18" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#sigmoidCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-double-">sigmoidCrossEntropy</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictionLogits,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                   <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
                   double&nbsp;labelSmoothing)</code>
<div class="block">Sigmoid cross entropy: applies the sigmoid activation function on the input logits (input "pre-sigmoid preductions")<br>
 and implements the binary cross entropy loss function.</div>
</td>
</tr>
<tr id="i19" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#softmaxCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">softmaxCrossEntropy</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;oneHotLabels,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logitPredictions,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</code>
<div class="block">Applies the softmax activation function to the input, then implement multi-class cross entropy:<br>
 <code>-sum_classes label[i] * log(p[c])</code> where <code>p = softmax(logits)</code><br>
 If <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#NONE"><code>LossReduce.NONE</code></a> is used, returned shape is [numExamples] out for [numExamples, numClasses] predicitons/labels;<br>
 otherwise, the output is a scalar.</div>
</td>
</tr>
<tr id="i20" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#softmaxCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-double-">softmaxCrossEntropy</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;oneHotLabels,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logitPredictions,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                   <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
                   double&nbsp;labelSmoothing)</code>
<div class="block">Applies the softmax activation function to the input, then implement multi-class cross entropy:<br>
 <code>-sum_classes label[i] * log(p[c])</code> where <code>p = softmax(logits)</code><br>
 If <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#NONE"><code>LossReduce.NONE</code></a> is used, returned shape is [numExamples] out for [numExamples, numClasses] predicitons/labels;<br>
 otherwise, the output is a scalar.</div>
</td>
</tr>
<tr id="i21" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#sparseSoftmaxCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">sparseSoftmaxCrossEntropy</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logits,
                         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;labels)</code>
<div class="block">As per softmaxCrossEntropy(String, SDVariable, SDVariable, LossReduce) but the labels variable<br>
 is represented as an integer array instead of the equivalent one-hot array.</div>
</td>
</tr>
<tr id="i22" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDLoss.html#weightedCrossEntropyWithLogits-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">weightedCrossEntropyWithLogits</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;targets,
                              <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;inputs,
                              <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</code>
<div class="block">Weighted cross entropy loss with logits<br></div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods.inherited.from.class.java.lang.Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;java.lang.<a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">Object</a></h3>
<code><a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#clone--" title="class or interface in java.lang">clone</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#equals-java.lang.Object-" title="class or interface in java.lang">equals</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#finalize--" title="class or interface in java.lang">finalize</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#getClass--" title="class or interface in java.lang">getClass</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#hashCode--" title="class or interface in java.lang">hashCode</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#notify--" title="class or interface in java.lang">notify</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#notifyAll--" title="class or interface in java.lang">notifyAll</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#toString--" title="class or interface in java.lang">toString</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#wait--" title="class or interface in java.lang">wait</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#wait-long-" title="class or interface in java.lang">wait</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#wait-long-int-" title="class or interface in java.lang">wait</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor.detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="NDLoss--">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>NDLoss</h4>
<pre>public&nbsp;NDLoss()</pre>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method.detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="absoluteDifference-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>absoluteDifference</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;absoluteDifference(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                                   <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce)</pre>
<div class="block">Absolute difference loss: <code>sum_i abs( label[i] - predictions[i] )</code><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="absoluteDifference-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>absoluteDifference</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;absoluteDifference(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</pre>
<div class="block">Absolute difference loss: <code>sum_i abs( label[i] - predictions[i] )</code><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="cosineDistance-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>cosineDistance</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;cosineDistance(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                               <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                               <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                               <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
                               int&nbsp;dimension)</pre>
<div class="block">Cosine distance loss: <code>1 - cosineSimilarity(x,y)</code> or <code>1 - sum_i label[i] * prediction[i]</code>, which is<br>
 equivalent to cosine distance when both the predictions and labels are normalized.<br>
 <b>Note</b>: This loss function assumes that both the predictions and labels are normalized to have unit l2 norm.<br>
 If this is not the case, you should normalize them first by dividing by norm2(String, SDVariable, boolean, int...)<br>
 along the cosine distance dimension (with keepDims=true).<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is use (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dd><code>dimension</code> - Dimension to perform the cosine distance over</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Cosine distance loss  (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="cosineDistance-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>cosineDistance</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;cosineDistance(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                               <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                               <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                               int&nbsp;dimension)</pre>
<div class="block">Cosine distance loss: <code>1 - cosineSimilarity(x,y)</code> or <code>1 - sum_i label[i] * prediction[i]</code>, which is<br>
 equivalent to cosine distance when both the predictions and labels are normalized.<br>
 <b>Note</b>: This loss function assumes that both the predictions and labels are normalized to have unit l2 norm.<br>
 If this is not the case, you should normalize them first by dividing by norm2(String, SDVariable, boolean, int...)<br>
 along the cosine distance dimension (with keepDims=true).<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is use (NUMERIC type)</dd>
<dd><code>dimension</code> - Dimension to perform the cosine distance over</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Cosine distance loss  (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="hingeLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>hingeLoss</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;hingeLoss(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                          <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce)</pre>
<div class="block">Hinge loss: a loss function used for training classifiers.<br>
 Implements <code>L = max(0, 1 - t * predictions)</code> where t is the label values after internally converting to {-1,1}<br>
 from the user specified {0,1}. Note that Labels should be provided with values {0,1}.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array. Each value should be 0.0 or 1.0 (internally -1 to 1 is used) (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="hingeLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>hingeLoss</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;hingeLoss(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</pre>
<div class="block">Hinge loss: a loss function used for training classifiers.<br>
 Implements <code>L = max(0, 1 - t * predictions)</code> where t is the label values after internally converting to {-1,1}<br>
 from the user specified {0,1}. Note that Labels should be provided with values {0,1}.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array. Each value should be 0.0 or 1.0 (internally -1 to 1 is used) (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="huberLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>huberLoss</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;huberLoss(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                          <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
                          double&nbsp;delta)</pre>
<div class="block">Huber loss function, used for robust regression. It is similar both squared error loss and absolute difference loss,<br>
 though is less sensitive to outliers than squared error.<br>
 Huber loss implements:<br>
 <pre><br>
 <code> L = 0.5 * (label[i] - predictions[i])^2 if abs(label[i] - predictions[i]) &lt; delta</code><br>
 <code> L = delta * abs(label[i] - predictions[i]) - 0.5 * delta^2 otherwise</code><br>
 </pre><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dd><code>delta</code> - Loss function delta value</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Huber loss (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="huberLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>huberLoss</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;huberLoss(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                          double&nbsp;delta)</pre>
<div class="block">Huber loss function, used for robust regression. It is similar both squared error loss and absolute difference loss,<br>
 though is less sensitive to outliers than squared error.<br>
 Huber loss implements:<br>
 <pre><br>
 <code> L = 0.5 * (label[i] - predictions[i])^2 if abs(label[i] - predictions[i]) &lt; delta</code><br>
 <code> L = delta * abs(label[i] - predictions[i]) - 0.5 * delta^2 otherwise</code><br>
 </pre><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>delta</code> - Loss function delta value</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Huber loss (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="l2Loss-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>l2Loss</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;l2Loss(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;var)</pre>
<div class="block">L2 loss: 1/2 * sum(x^2)<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>var</code> - Variable to calculate L2 loss of (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output L2 loss (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="logLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>logLoss</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logLoss(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                        <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                        <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                        <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
                        double&nbsp;epsilon)</pre>
<div class="block">Log loss, i.e., binary cross entropy loss, usually used for binary multi-label classification. Implements:<br>
 <code>-1/numExamples * sum_i (labels[i] * log(predictions[i] + epsilon) + (1-labels[i]) * log(1-predictions[i] + epsilon))</code><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dd><code>epsilon</code> - epsilon</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Log loss  (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="logLoss-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>logLoss</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logLoss(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                        <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions)</pre>
<div class="block">Log loss, i.e., binary cross entropy loss, usually used for binary multi-label classification. Implements:<br>
 <code>-1/numExamples * sum_i (labels[i] * log(predictions[i] + epsilon) + (1-labels[i]) * log(1-predictions[i] + epsilon))</code><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Log loss  (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="logPoisson-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-boolean-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>logPoisson</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logPoisson(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                           <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                           <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                           <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
                           boolean&nbsp;full)</pre>
<div class="block">Log poisson loss: a loss function used for training classifiers.<br>
 Implements <code>L = exp(c) - z * c</code> where c is log(predictions) and z is labels.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array. Each value should be 0.0 or 1.0 (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (has to be log(x) of actual predictions) (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dd><code>full</code> - Boolean flag. true for logPoissonFull, false for logPoisson</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="logPoisson-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>logPoisson</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logPoisson(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                           <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                           <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                           boolean&nbsp;full)</pre>
<div class="block">Log poisson loss: a loss function used for training classifiers.<br>
 Implements <code>L = exp(c) - z * c</code> where c is log(predictions) and z is labels.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array. Each value should be 0.0 or 1.0 (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (has to be log(x) of actual predictions) (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>full</code> - Boolean flag. true for logPoissonFull, false for logPoisson</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="meanPairwiseSquaredError-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>meanPairwiseSquaredError</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;meanPairwiseSquaredError(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                                         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                                         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                                         <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce)</pre>
<div class="block">Mean pairwise squared error.<br>
 MPWSE loss calculates the difference between pairs of consecutive elements in the predictions and labels arrays.<br>
 For example, if predictions = [p0, p1, p2] and labels are [l0, l1, l2] then MPWSE is:<br>
 <code>[((p0-p1) - (l0-l1))^2 + ((p0-p2) - (l0-l2))^2 + ((p1-p2) - (l1-l2))^2] / 3</code><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used. Must be either null, scalar, or have shape [batchSize] (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable, scalar output (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="meanPairwiseSquaredError-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>meanPairwiseSquaredError</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;meanPairwiseSquaredError(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                                         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                                         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</pre>
<div class="block">Mean pairwise squared error.<br>
 MPWSE loss calculates the difference between pairs of consecutive elements in the predictions and labels arrays.<br>
 For example, if predictions = [p0, p1, p2] and labels are [l0, l1, l2] then MPWSE is:<br>
 <code>[((p0-p1) - (l0-l1))^2 + ((p0-p2) - (l0-l2))^2 + ((p1-p2) - (l1-l2))^2] / 3</code><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used. Must be either null, scalar, or have shape [batchSize] (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable, scalar output (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="meanSquaredError-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>meanSquaredError</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;meanSquaredError(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                                 <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                                 <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                                 <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce)</pre>
<div class="block">Mean squared error loss function. Implements <code>(label[i] - prediction[i])^2</code> - i.e., squared error on a per-element basis.<br>
 When averaged (using <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_WEIGHT"><code>LossReduce.MEAN_BY_WEIGHT</code></a> or <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a> (the default))<br>
 this is the mean squared error loss function.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="meanSquaredError-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>meanSquaredError</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;meanSquaredError(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                                 <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictions,
                                 <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</pre>
<div class="block">Mean squared error loss function. Implements <code>(label[i] - prediction[i])^2</code> - i.e., squared error on a per-element basis.<br>
 When averaged (using <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_WEIGHT"><code>LossReduce.MEAN_BY_WEIGHT</code></a> or <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a> (the default))<br>
 this is the mean squared error loss function.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictions</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="sigmoidCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sigmoidCrossEntropy</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;sigmoidCrossEntropy(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictionLogits,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                                    <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
                                    double&nbsp;labelSmoothing)</pre>
<div class="block">Sigmoid cross entropy: applies the sigmoid activation function on the input logits (input "pre-sigmoid preductions")<br>
 and implements the binary cross entropy loss function. This implementation is numerically more stable than using<br>
 standard (but separate) sigmoid activation function and log loss (binary cross entropy) loss function.<br>
 Implements:<br>
 <code>-1/numExamples * sum_i (labels[i] * log(sigmoid(logits[i])) + (1-labels[i]) * log(1-sigmoid(logits[i])))</code><br>
 though this is done in a mathematically equivalent but more numerical stable form.<br>
 <br>
 When label smoothing is > 0, the following label smoothing is used:<br>
 <pre><br>
 <code> numClasses = labels.size(1);&lt;br&gt;
 label = (1.0 - labelSmoothing) * label + 0.5 * labelSmoothing</code><br>
 </pre><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictionLogits</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dd><code>labelSmoothing</code> - Label smoothing value. Default value: 0</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="sigmoidCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sigmoidCrossEntropy</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;sigmoidCrossEntropy(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;label,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;predictionLogits,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</pre>
<div class="block">Sigmoid cross entropy: applies the sigmoid activation function on the input logits (input "pre-sigmoid preductions")<br>
 and implements the binary cross entropy loss function. This implementation is numerically more stable than using<br>
 standard (but separate) sigmoid activation function and log loss (binary cross entropy) loss function.<br>
 Implements:<br>
 <code>-1/numExamples * sum_i (labels[i] * log(sigmoid(logits[i])) + (1-labels[i]) * log(1-sigmoid(logits[i])))</code><br>
 though this is done in a mathematically equivalent but more numerical stable form.<br>
 <br>
 When label smoothing is > 0, the following label smoothing is used:<br>
 <pre><br>
 <code> numClasses = labels.size(1);&lt;br&gt;
 label = (1.0 - labelSmoothing) * label + 0.5 * labelSmoothing</code><br>
 </pre><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>label</code> - Label array (NUMERIC type)</dd>
<dd><code>predictionLogits</code> - Predictions array (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="softmaxCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.autodiff.loss.LossReduce-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>softmaxCrossEntropy</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;softmaxCrossEntropy(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;oneHotLabels,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logitPredictions,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                                    <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss">LossReduce</a>&nbsp;lossReduce,
                                    double&nbsp;labelSmoothing)</pre>
<div class="block">Applies the softmax activation function to the input, then implement multi-class cross entropy:<br>
 <code>-sum_classes label[i] * log(p[c])</code> where <code>p = softmax(logits)</code><br>
 If <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#NONE"><code>LossReduce.NONE</code></a> is used, returned shape is [numExamples] out for [numExamples, numClasses] predicitons/labels;<br>
 otherwise, the output is a scalar.<br>
 <p><br>
 When label smoothing is > 0, the following label smoothing is used:<br>
 <pre><br>
 <code> numClasses = labels.size(1);&lt;br&gt;
 oneHotLabel = (1.0 - labelSmoothing) * oneHotLabels + labelSmoothing/numClasses</code><br>
 </pre><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>oneHotLabels</code> - Label array. Should be one-hot per example and same shape as predictions (for example, [mb, nOut]) (NUMERIC type)</dd>
<dd><code>logitPredictions</code> - Predictions array (pre-softmax) (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dd><code>lossReduce</code> - Reduction type for the loss. See <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html" title="enum in org.nd4j.autodiff.loss"><code>LossReduce</code></a> for more details. Default: <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#MEAN_BY_NONZERO_WEIGHT_COUNT"><code>LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT</code></a></dd>
<dd><code>labelSmoothing</code> - Label smoothing value. Default value: 0</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="softmaxCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>softmaxCrossEntropy</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;softmaxCrossEntropy(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;oneHotLabels,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logitPredictions,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</pre>
<div class="block">Applies the softmax activation function to the input, then implement multi-class cross entropy:<br>
 <code>-sum_classes label[i] * log(p[c])</code> where <code>p = softmax(logits)</code><br>
 If <a href="../../../../../org/nd4j/autodiff/loss/LossReduce.html#NONE"><code>LossReduce.NONE</code></a> is used, returned shape is [numExamples] out for [numExamples, numClasses] predicitons/labels;<br>
 otherwise, the output is a scalar.<br>
 <p><br>
 When label smoothing is > 0, the following label smoothing is used:<br>
 <pre><br>
 <code> numClasses = labels.size(1);&lt;br&gt;
 oneHotLabel = (1.0 - labelSmoothing) * oneHotLabels + labelSmoothing/numClasses</code><br>
 </pre><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>oneHotLabels</code> - Label array. Should be one-hot per example and same shape as predictions (for example, [mb, nOut]) (NUMERIC type)</dd>
<dd><code>logitPredictions</code> - Predictions array (pre-softmax) (NUMERIC type)</dd>
<dd><code>weights</code> - Weights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="sparseSoftmaxCrossEntropy-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sparseSoftmaxCrossEntropy</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;sparseSoftmaxCrossEntropy(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logits,
                                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;labels)</pre>
<div class="block">As per softmaxCrossEntropy(String, SDVariable, SDVariable, LossReduce) but the labels variable<br>
 is represented as an integer array instead of the equivalent one-hot array.<br>
 i.e., if logits are rank N, then labels have rank N-1<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>logits</code> - Logits array ("pre-softmax activations") (NUMERIC type)</dd>
<dd><code>labels</code> - Labels array. Must be an integer type. (INT type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Softmax cross entropy (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="weightedCrossEntropyWithLogits-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>weightedCrossEntropyWithLogits</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weightedCrossEntropyWithLogits(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;targets,
                                               <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;inputs,
                                               <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights)</pre>
<div class="block">Weighted cross entropy loss with logits<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>targets</code> - targets array (NUMERIC type)</dd>
<dd><code>inputs</code> - input array (NUMERIC type)</dd>
<dd><code>weights</code> - eights array. May be null. If null, a weight of 1.0 is used (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Loss variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar.bottom">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.bottom" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.bottom.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="class-use/NDLoss.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../../index-all.html">Index</a></li>
<li><a href="../../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../../org/nd4j/linalg/factory/ops/NDLinalg.html" title="class in org.nd4j.linalg.factory.ops"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../../../../org/nd4j/linalg/factory/ops/NDMath.html" title="class in org.nd4j.linalg.factory.ops"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../../index.html?org/nd4j/linalg/factory/ops/NDLoss.html" target="_top">Frames</a></li>
<li><a href="NDLoss.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
<p class="legalCopy"><small>Copyright &#169; 2020. All rights reserved.</small></p>
</body>
</html>
