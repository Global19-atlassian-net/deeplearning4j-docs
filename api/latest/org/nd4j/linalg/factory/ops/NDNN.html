<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="de">
<head>
<!-- Generated by javadoc (1.8.0_151) on Sat May 16 12:24:21 CEST 2020 -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>NDNN (deeplearning4j 1.0.0-beta7 API)</title>
<meta name="date" content="2020-05-16">
<link rel="stylesheet" type="text/css" href="../../../../../stylesheet.css" title="Style">
<script type="text/javascript" src="../../../../../script.js"></script>
</head>
<body>
<script type="text/javascript"><!--
    try {
        if (location.href.indexOf('is-external=true') == -1) {
            parent.document.title="NDNN (deeplearning4j 1.0.0-beta7 API)";
        }
    }
    catch(err) {
    }
//-->
var methods = {"i0":10,"i1":10,"i2":10,"i3":10,"i4":10,"i5":10,"i6":10,"i7":10,"i8":10,"i9":10,"i10":10,"i11":10,"i12":10,"i13":10,"i14":10,"i15":10,"i16":10,"i17":10,"i18":10,"i19":10,"i20":10,"i21":10,"i22":10,"i23":10,"i24":10,"i25":10,"i26":10,"i27":10,"i28":10,"i29":10,"i30":10,"i31":10,"i32":10,"i33":10,"i34":10,"i35":10,"i36":10};
var tabs = {65535:["t0","All Methods"],2:["t2","Instance Methods"],8:["t4","Concrete Methods"]};
var altColor = "altColor";
var rowColor = "rowColor";
var tableTab = "tableTab";
var activeTableTab = "activeTableTab";
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar.top">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.top" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.top.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="class-use/NDNN.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../../index-all.html">Index</a></li>
<li><a href="../../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../../org/nd4j/linalg/factory/ops/NDMath.html" title="class in org.nd4j.linalg.factory.ops"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../../../../org/nd4j/linalg/factory/ops/NDRandom.html" title="class in org.nd4j.linalg.factory.ops"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../../index.html?org/nd4j/linalg/factory/ops/NDNN.html" target="_top">Frames</a></li>
<li><a href="NDNN.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.nd4j.linalg.factory.ops</div>
<h2 title="Class NDNN" class="title">Class NDNN</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li><a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">java.lang.Object</a></li>
<li>
<ul class="inheritance">
<li>org.nd4j.linalg.factory.ops.NDNN</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<hr>
<br>
<pre>public class <span class="typeNameLabel">NDNN</span>
extends <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">Object</a></pre>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor.summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="memberSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#NDNN--">NDNN</a></span>()</code>&nbsp;</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method.summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="memberSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span id="t0" class="activeTableTab"><span>All Methods</span><span class="tabEnd">&nbsp;</span></span><span id="t2" class="tableTab"><span><a href="javascript:show(2);">Instance Methods</a></span><span class="tabEnd">&nbsp;</span></span><span id="t4" class="tableTab"><span><a href="javascript:show(8);">Concrete Methods</a></span><span class="tabEnd">&nbsp;</span></span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr id="i0" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#batchNorm-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-double-int...-">batchNorm</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;mean,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;variance,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;gamma,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;beta,
         double&nbsp;epsilon,
         int...&nbsp;axis)</code>
<div class="block">Neural network batch normalization operation.</div>
</td>
</tr>
<tr id="i1" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#biasAdd-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-">biasAdd</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
       <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;bias,
       boolean&nbsp;nchw)</code>
<div class="block">Bias addition operation: a special case of addition, typically used with CNN 4D activations and a 1D bias vector<br></div>
</td>
</tr>
<tr id="i2" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#cReLU-org.nd4j.linalg.api.ndarray.INDArray-">cReLU</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Concatenates a ReLU which selects only the positive part of the activation with a ReLU which selects only the negative part of the activation.</div>
</td>
</tr>
<tr id="i3" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#dotProductAttention-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-">dotProductAttention</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;queries,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;keys,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;values,
                   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;mask,
                   boolean&nbsp;scaled)</code>
<div class="block">This operation performs dot product attention on the given timeseries input with the given queries<br>
 out = sum(similarity(k_i, q) * v_i)<br>
 <br>
 similarity(k, q) = softmax(k * q) where x * q is the dot product of x and q<br>
 <br>
 Optionally with normalization step:<br>
 similarity(k, q) = softmax(k * q / sqrt(size(q))<br>
 <br>
 See also "Attention is all you need" (https://arxiv.org/abs/1706.03762, p. 4, eq. 1)<br>
 <br>
 Note: This supports multiple queries at once, if only one query is available the queries vector still has to<br>
 be 3D but can have queryCount = 1<br>
 <br>
 Note: keys and values usually is the same array.</div>
</td>
</tr>
<tr id="i4" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#dropout-org.nd4j.linalg.api.ndarray.INDArray-double-">dropout</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
       double&nbsp;inputRetainProbability)</code>
<div class="block">Dropout operation<br></div>
</td>
</tr>
<tr id="i5" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#elu-org.nd4j.linalg.api.ndarray.INDArray-">elu</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise exponential linear unit (ELU) function:<br>
 out = x if x > 0<br>
 out = a * (exp(x) - 1) if x <= 0<br>
 with constant a = 1.0<br>
 <br>
 See: <a href="https://arxiv.org/abs/1511.07289">https://arxiv.org/abs/1511.07289</a><br></div>
</td>
</tr>
<tr id="i6" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#gelu-org.nd4j.linalg.api.ndarray.INDArray-">gelu</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">GELU activation function - Gaussian Error Linear Units<br>
 For more details, see <i>Gaussian Error Linear Units (GELUs)</i> - <a href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a><br>
 This method uses the sigmoid approximation<br></div>
</td>
</tr>
<tr id="i7" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#hardSigmoid-org.nd4j.linalg.api.ndarray.INDArray-">hardSigmoid</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise hard sigmoid function:<br>
 out[i] = 0 if in[i] <= -2.5<br>
 out[1] = 0.2*in[i]+0.5 if -2.5 < in[i] < 2.5<br>
 out[i] = 1 if in[i] >= 2.5<br></div>
</td>
</tr>
<tr id="i8" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#hardTanh-org.nd4j.linalg.api.ndarray.INDArray-">hardTanh</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise hard tanh function:<br>
 out[i] = -1 if in[i] <= -1<br>
 out[1] = in[i] if -1 < in[i] < 1<br>
 out[i] = 1 if in[i] >= 1<br></div>
</td>
</tr>
<tr id="i9" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#hardTanhDerivative-org.nd4j.linalg.api.ndarray.INDArray-">hardTanhDerivative</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Derivative (dOut/dIn) of the element-wise hard Tanh function - hardTanh(INDArray)<br></div>
</td>
</tr>
<tr id="i10" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#layerNorm-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-int...-">layerNorm</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;gain,
         boolean&nbsp;channelsFirst,
         int...&nbsp;dimensions)</code>
<div class="block">Apply Layer Normalization<br>
 <br>
 y = gain * standardize(x) + bias<br></div>
</td>
</tr>
<tr id="i11" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#layerNorm-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-int...-">layerNorm</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;gain,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;bias,
         boolean&nbsp;channelsFirst,
         int...&nbsp;dimensions)</code>
<div class="block">Apply Layer Normalization<br>
 <br>
 y = gain * standardize(x) + bias<br></div>
</td>
</tr>
<tr id="i12" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#leakyRelu-org.nd4j.linalg.api.ndarray.INDArray-double-">leakyRelu</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
         double&nbsp;alpha)</code>
<div class="block">Element-wise leaky ReLU function:<br>
 out = x if x >= 0.0<br>
 out = alpha * x if x < cutoff<br>
 Alpha value is most commonly set to 0.01<br></div>
</td>
</tr>
<tr id="i13" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#leakyReluDerivative-org.nd4j.linalg.api.ndarray.INDArray-double-">leakyReluDerivative</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                   double&nbsp;alpha)</code>
<div class="block">Leaky ReLU derivative: dOut/dIn given input.</div>
</td>
</tr>
<tr id="i14" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#linear-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">linear</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
      <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
      <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;bias)</code>
<div class="block">Linear layer operation: out = mmul(in,w) + bias<br>
 Note that bias array is optional<br></div>
</td>
</tr>
<tr id="i15" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#logSigmoid-org.nd4j.linalg.api.ndarray.INDArray-">logSigmoid</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise sigmoid function: out[i] = log(sigmoid(in[i]))<br></div>
</td>
</tr>
<tr id="i16" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#logSoftmax-org.nd4j.linalg.api.ndarray.INDArray-">logSoftmax</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Log softmax activation<br></div>
</td>
</tr>
<tr id="i17" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#logSoftmax-org.nd4j.linalg.api.ndarray.INDArray-int-">logSoftmax</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
          int&nbsp;dimension)</code>
<div class="block">Log softmax activation<br></div>
</td>
</tr>
<tr id="i18" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#multiHeadDotProductAttention-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-">multiHeadDotProductAttention</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;queries,
                            <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;keys,
                            <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;values,
                            <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;Wq,
                            <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;Wk,
                            <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;Wv,
                            <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;Wo,
                            <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;mask,
                            boolean&nbsp;scaled)</code>
<div class="block">This performs multi-headed dot product attention on the given timeseries input<br>
 out = concat(head_1, head_2, ..., head_n) * Wo<br>
 head_i = dot_product_attention(Wq_i*q, Wk_i*k, Wv_i*v)<br>
 <br>
 Optionally with normalization when calculating the attention for each head.</div>
</td>
</tr>
<tr id="i19" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#pad-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-double-">pad</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;padding,
   double&nbsp;constant)</code>
<div class="block">Padding operation <br></div>
</td>
</tr>
<tr id="i20" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#pad-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.enums.PadMode-double-">pad</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
   <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;padding,
   <a href="../../../../../org/nd4j/enums/PadMode.html" title="enum in org.nd4j.enums">PadMode</a>&nbsp;PadMode,
   double&nbsp;constant)</code>
<div class="block">Padding operation <br></div>
</td>
</tr>
<tr id="i21" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#preciseGelu-org.nd4j.linalg.api.ndarray.INDArray-">preciseGelu</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">GELU activation function - Gaussian Error Linear Units<br>
 For more details, see <i>Gaussian Error Linear Units (GELUs)</i> - <a href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a><br>
 This method uses the precise method<br></div>
</td>
</tr>
<tr id="i22" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#prelu-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-int...-">prelu</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
     <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;alpha,
     int...&nbsp;sharedAxes)</code>
<div class="block">PReLU (Parameterized Rectified Linear Unit) operation.</div>
</td>
</tr>
<tr id="i23" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#relu-org.nd4j.linalg.api.ndarray.INDArray-double-">relu</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
    double&nbsp;cutoff)</code>
<div class="block">Element-wise rectified linear function with specified cutoff:<br>
 out[i] = in[i] if in[i] >= cutoff<br>
 out[i] = 0 otherwise<br></div>
</td>
</tr>
<tr id="i24" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#relu6-org.nd4j.linalg.api.ndarray.INDArray-double-">relu6</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
     double&nbsp;cutoff)</code>
<div class="block">Element-wise "rectified linear 6" function with specified cutoff:<br>
 out[i] = min(max(in, cutoff), 6)<br></div>
</td>
</tr>
<tr id="i25" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#reluLayer-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">reluLayer</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
         <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;bias)</code>
<div class="block">ReLU (Rectified Linear Unit) layer operation: out = relu(mmul(in,w) + bias)<br>
 Note that bias array is optional<br></div>
</td>
</tr>
<tr id="i26" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#selu-org.nd4j.linalg.api.ndarray.INDArray-">selu</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise SeLU function - Scaled exponential Lineal Unit: see <a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a><br>
 <br>
 out[i] = scale * alpha * (exp(in[i])-1) if in[i]>0, or 0 if in[i] <= 0<br>
 Uses default scale and alpha values.</div>
</td>
</tr>
<tr id="i27" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#sigmoid-org.nd4j.linalg.api.ndarray.INDArray-">sigmoid</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise sigmoid function: out[i] = 1.0/(1+exp(-in[i]))<br></div>
</td>
</tr>
<tr id="i28" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#sigmoidDerivative-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">sigmoidDerivative</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                 <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;wrt)</code>
<div class="block">Element-wise sigmoid function derivative: dL/dIn given input and dL/dOut<br></div>
</td>
</tr>
<tr id="i29" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#softmax-org.nd4j.linalg.api.ndarray.INDArray-">softmax</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Softmax activation, along the specified dimension<br></div>
</td>
</tr>
<tr id="i30" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#softmax-org.nd4j.linalg.api.ndarray.INDArray-int-">softmax</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
       int&nbsp;dimension)</code>
<div class="block">Softmax activation, along the specified dimension<br></div>
</td>
</tr>
<tr id="i31" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#softmaxDerivative-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-int-">softmaxDerivative</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                 <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;wrt,
                 int&nbsp;dimension)</code>
<div class="block">Softmax derivative function<br></div>
</td>
</tr>
<tr id="i32" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#softplus-org.nd4j.linalg.api.ndarray.INDArray-">softplus</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise softplus function: out = log(exp(x) + 1)<br></div>
</td>
</tr>
<tr id="i33" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#softsign-org.nd4j.linalg.api.ndarray.INDArray-">softsign</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise softsign function: out = x / (abs(x) + 1)<br></div>
</td>
</tr>
<tr id="i34" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#softsignDerivative-org.nd4j.linalg.api.ndarray.INDArray-">softsignDerivative</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise derivative (dOut/dIn) of the softsign function softsign(INDArray)<br></div>
</td>
</tr>
<tr id="i35" class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#swish-org.nd4j.linalg.api.ndarray.INDArray-">swish</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Element-wise "swish" function: out = x * sigmoid(b*x) with b=1.0<br>
 See: <a href="https://arxiv.org/abs/1710.05941">https://arxiv.org/abs/1710.05941</a><br></div>
</td>
</tr>
<tr id="i36" class="altColor">
<td class="colFirst"><code><a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../../org/nd4j/linalg/factory/ops/NDNN.html#tanh-org.nd4j.linalg.api.ndarray.INDArray-">tanh</a></span>(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</code>
<div class="block">Elementwise tanh (hyperbolic tangent) operation: out = tanh(x)<br></div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods.inherited.from.class.java.lang.Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;java.lang.<a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">Object</a></h3>
<code><a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#clone--" title="class or interface in java.lang">clone</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#equals-java.lang.Object-" title="class or interface in java.lang">equals</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#finalize--" title="class or interface in java.lang">finalize</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#getClass--" title="class or interface in java.lang">getClass</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#hashCode--" title="class or interface in java.lang">hashCode</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#notify--" title="class or interface in java.lang">notify</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#notifyAll--" title="class or interface in java.lang">notifyAll</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#toString--" title="class or interface in java.lang">toString</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#wait--" title="class or interface in java.lang">wait</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#wait-long-" title="class or interface in java.lang">wait</a>, <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html?is-external=true#wait-long-int-" title="class or interface in java.lang">wait</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor.detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="NDNN--">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>NDNN</h4>
<pre>public&nbsp;NDNN()</pre>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method.detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="cReLU-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>cReLU</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;cReLU(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Concatenates a ReLU which selects only the positive part of the activation with a ReLU which selects only the negative part of the activation. Note that as a result this non-linearity doubles the depth of the activations.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="batchNorm-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-double-int...-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>batchNorm</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;batchNorm(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;mean,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;variance,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;gamma,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;beta,
                          double&nbsp;epsilon,
                          int...&nbsp;axis)</pre>
<div class="block">Neural network batch normalization operation.<br>
 For details, see <a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - Input variable. (NUMERIC type)</dd>
<dd><code>mean</code> - Mean value. For 1d axis, this should match input.size(axis) (NUMERIC type)</dd>
<dd><code>variance</code> - Variance value. For 1d axis, this should match input.size(axis) (NUMERIC type)</dd>
<dd><code>gamma</code> - Gamma value. For 1d axis, this should match input.size(axis) (NUMERIC type)</dd>
<dd><code>beta</code> - Beta value. For 1d axis, this should match input.size(axis) (NUMERIC type)</dd>
<dd><code>epsilon</code> - Epsilon constant for numerical stability (to avoid division by 0)</dd>
<dd><code>axis</code> - For 2d CNN activations: 1 for NCHW format activations, or 3 for NHWC format activations.
 For 3d CNN activations: 1 for NCDHW format, 4 for NDHWC
 For 1d/RNN activations: 1 for NCW format, 2 for NWC (Size: AtLeast(min=1))</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output variable for batch normalization (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="biasAdd-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>biasAdd</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;biasAdd(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                        <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;bias,
                        boolean&nbsp;nchw)</pre>
<div class="block">Bias addition operation: a special case of addition, typically used with CNN 4D activations and a 1D bias vector<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - 4d input variable (NUMERIC type)</dd>
<dd><code>bias</code> - 1d bias (NUMERIC type)</dd>
<dd><code>nchw</code> - The format - nchw=true means [minibatch, channels, height, width] format; nchw=false - [minibatch, height, width, channels].
 Unused for 2d inputs</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable, after applying bias add operation (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="dotProductAttention-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>dotProductAttention</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;dotProductAttention(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;queries,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;keys,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;values,
                                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;mask,
                                    boolean&nbsp;scaled)</pre>
<div class="block">This operation performs dot product attention on the given timeseries input with the given queries<br>
 out = sum(similarity(k_i, q) * v_i)<br>
 <br>
 similarity(k, q) = softmax(k * q) where x * q is the dot product of x and q<br>
 <br>
 Optionally with normalization step:<br>
 similarity(k, q) = softmax(k * q / sqrt(size(q))<br>
 <br>
 See also "Attention is all you need" (https://arxiv.org/abs/1706.03762, p. 4, eq. 1)<br>
 <br>
 Note: This supports multiple queries at once, if only one query is available the queries vector still has to<br>
 be 3D but can have queryCount = 1<br>
 <br>
 Note: keys and values usually is the same array. If you want to use it as the same array, simply pass it for<br>
 both.<br>
 <br>
 Note: Queries, keys and values must either be all rank 3 or all rank 4 arrays. Mixing them doesn't work. The<br>
 output rank will depend on the input rank.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>queries</code> - input 3D array "queries" of shape [batchSize, featureKeys, queryCount]
 or 4D array of shape [batchSize, numHeads, featureKeys, queryCount] (NUMERIC type)</dd>
<dd><code>keys</code> - input 3D array "keys" of shape [batchSize, featureKeys, timesteps]
 or 4D array of shape [batchSize, numHeads, featureKeys, timesteps] (NUMERIC type)</dd>
<dd><code>values</code> - input 3D array "values" of shape [batchSize, featureValues, timesteps]
 or 4D array of shape [batchSize, numHeads, featureValues, timesteps] (NUMERIC type)</dd>
<dd><code>mask</code> - OPTIONAL; array that defines which values should be skipped of shape [batchSize, timesteps] (NUMERIC type)</dd>
<dd><code>scaled</code> - normalization, false -> do not apply normalization, true -> apply normalization</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output  Attention result arrays of shape [batchSize, featureValues, queryCount] or [batchSize, numHeads, featureValues, queryCount],
 (optionally) Attention Weights of shape [batchSize, timesteps, queryCount] or [batchSize, numHeads, timesteps, queryCount] (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="dropout-org.nd4j.linalg.api.ndarray.INDArray-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>dropout</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;dropout(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                        double&nbsp;inputRetainProbability)</pre>
<div class="block">Dropout operation<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - Input array (NUMERIC type)</dd>
<dd><code>inputRetainProbability</code> - Probability of retaining an input (set to 0 with probability 1-p)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="elu-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>elu</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;elu(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise exponential linear unit (ELU) function:<br>
 out = x if x > 0<br>
 out = a * (exp(x) - 1) if x <= 0<br>
 with constant a = 1.0<br>
 <p><br>
 See: <a href="https://arxiv.org/abs/1511.07289">https://arxiv.org/abs/1511.07289</a><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="gelu-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>gelu</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;gelu(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">GELU activation function - Gaussian Error Linear Units<br>
 For more details, see <i>Gaussian Error Linear Units (GELUs)</i> - <a href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a><br>
 This method uses the sigmoid approximation<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="hardSigmoid-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>hardSigmoid</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;hardSigmoid(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise hard sigmoid function:<br>
 out[i] = 0 if in[i] <= -2.5<br>
 out[1] = 0.2*in[i]+0.5 if -2.5 < in[i] < 2.5<br>
 out[i] = 1 if in[i] >= 2.5<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="hardTanh-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>hardTanh</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;hardTanh(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise hard tanh function:<br>
 out[i] = -1 if in[i] <= -1<br>
 out[1] = in[i] if -1 < in[i] < 1<br>
 out[i] = 1 if in[i] >= 1<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="hardTanhDerivative-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>hardTanhDerivative</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;hardTanhDerivative(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Derivative (dOut/dIn) of the element-wise hard Tanh function - hardTanh(INDArray)<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="layerNorm-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-int...-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>layerNorm</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;layerNorm(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;gain,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;bias,
                          boolean&nbsp;channelsFirst,
                          int...&nbsp;dimensions)</pre>
<div class="block">Apply Layer Normalization<br>
 <br>
 y = gain * standardize(x) + bias<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - Input variable (NUMERIC type)</dd>
<dd><code>gain</code> - Gain (NUMERIC type)</dd>
<dd><code>bias</code> - Bias (NUMERIC type)</dd>
<dd><code>channelsFirst</code> - For 2D input - unused. True for NCHW (minibatch, channels, height, width), false for NHWC data</dd>
<dd><code>dimensions</code> - Dimensions to perform layer norm over - dimension=1 for 2d/MLP data, dimension=1,2,3 for CNNs (Size: AtLeast(min=1))</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="layerNorm-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-int...-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>layerNorm</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;layerNorm(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;gain,
                          boolean&nbsp;channelsFirst,
                          int...&nbsp;dimensions)</pre>
<div class="block">Apply Layer Normalization<br>
 <br>
 y = gain * standardize(x) + bias<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - Input variable (NUMERIC type)</dd>
<dd><code>gain</code> - Gain (NUMERIC type)</dd>
<dd><code>channelsFirst</code> - For 2D input - unused. True for NCHW (minibatch, channels, height, width), false for NHWC data</dd>
<dd><code>dimensions</code> - Dimensions to perform layer norm over - dimension=1 for 2d/MLP data, dimension=1,2,3 for CNNs (Size: AtLeast(min=1))</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="leakyRelu-org.nd4j.linalg.api.ndarray.INDArray-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>leakyRelu</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;leakyRelu(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                          double&nbsp;alpha)</pre>
<div class="block">Element-wise leaky ReLU function:<br>
 out = x if x >= 0.0<br>
 out = alpha * x if x < cutoff<br>
 Alpha value is most commonly set to 0.01<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dd><code>alpha</code> - Cutoff - commonly 0.01</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="leakyReluDerivative-org.nd4j.linalg.api.ndarray.INDArray-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>leakyReluDerivative</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;leakyReluDerivative(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                                    double&nbsp;alpha)</pre>
<div class="block">Leaky ReLU derivative: dOut/dIn given input.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dd><code>alpha</code> - Cutoff - commonly 0.01</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="linear-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>linear</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;linear(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                       <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                       <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;bias)</pre>
<div class="block">Linear layer operation: out = mmul(in,w) + bias<br>
 Note that bias array is optional<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - Input data (NUMERIC type)</dd>
<dd><code>weights</code> - Weights variable, shape [nIn, nOut] (NUMERIC type)</dd>
<dd><code>bias</code> - Optional bias variable (may be null) (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="logSigmoid-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>logSigmoid</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logSigmoid(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise sigmoid function: out[i] = log(sigmoid(in[i]))<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="logSoftmax-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>logSoftmax</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logSoftmax(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Log softmax activation<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output  (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="logSoftmax-org.nd4j.linalg.api.ndarray.INDArray-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>logSoftmax</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;logSoftmax(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                           int&nbsp;dimension)</pre>
<div class="block">Log softmax activation<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input (NUMERIC type)</dd>
<dd><code>dimension</code> - Dimension along which to apply log softmax</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output - log(softmax(input)) (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="multiHeadDotProductAttention-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-boolean-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>multiHeadDotProductAttention</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;multiHeadDotProductAttention(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;queries,
                                             <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;keys,
                                             <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;values,
                                             <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;Wq,
                                             <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;Wk,
                                             <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;Wv,
                                             <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;Wo,
                                             <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;mask,
                                             boolean&nbsp;scaled)</pre>
<div class="block">This performs multi-headed dot product attention on the given timeseries input<br>
 out = concat(head_1, head_2, ..., head_n) * Wo<br>
 head_i = dot_product_attention(Wq_i*q, Wk_i*k, Wv_i*v)<br>
 <br>
 Optionally with normalization when calculating the attention for each head.<br>
 <br>
 See also "Attention is all you need" (https://arxiv.org/abs/1706.03762, pp. 4,5, "3.2.2 Multi-Head Attention")<br>
 <br>
 This makes use of dot_product_attention OP support for rank 4 inputs.<br>
 see dotProductAttention(INDArray, INDArray, INDArray, INDArray, boolean, boolean)<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>queries</code> - input 3D array "queries" of shape [batchSize, featureKeys, queryCount] (NUMERIC type)</dd>
<dd><code>keys</code> - input 3D array "keys" of shape [batchSize, featureKeys, timesteps] (NUMERIC type)</dd>
<dd><code>values</code> - input 3D array "values" of shape [batchSize, featureValues, timesteps] (NUMERIC type)</dd>
<dd><code>Wq</code> - input query projection weights of shape [numHeads, projectedKeys, featureKeys] (NUMERIC type)</dd>
<dd><code>Wk</code> - input key projection weights of shape [numHeads, projectedKeys, featureKeys] (NUMERIC type)</dd>
<dd><code>Wv</code> - input value projection weights of shape [numHeads, projectedValues, featureValues] (NUMERIC type)</dd>
<dd><code>Wo</code> - output projection weights of shape [numHeads * projectedValues, outSize] (NUMERIC type)</dd>
<dd><code>mask</code> - OPTIONAL; array that defines which values should be skipped of shape [batchSize, timesteps] (NUMERIC type)</dd>
<dd><code>scaled</code> - normalization, false -> do not apply normalization, true -> apply normalization</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Attention result arrays of shape [batchSize, outSize, queryCount]
 (optionally) Attention Weights of shape [batchSize, numHeads, timesteps, queryCount] (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="pad-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.enums.PadMode-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>pad</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;pad(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;padding,
                    <a href="../../../../../org/nd4j/enums/PadMode.html" title="enum in org.nd4j.enums">PadMode</a>&nbsp;PadMode,
                    double&nbsp;constant)</pre>
<div class="block">Padding operation <br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - Input tensor (NUMERIC type)</dd>
<dd><code>padding</code> - Padding value (NUMERIC type)</dd>
<dd><code>PadMode</code> - Padding format</dd>
<dd><code>constant</code> - Padding constant</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Padded input (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="pad-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>pad</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;pad(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                    <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;padding,
                    double&nbsp;constant)</pre>
<div class="block">Padding operation <br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - Input tensor (NUMERIC type)</dd>
<dd><code>padding</code> - Padding value (NUMERIC type)</dd>
<dd><code>constant</code> - Padding constant</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Padded input (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="preciseGelu-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>preciseGelu</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;preciseGelu(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">GELU activation function - Gaussian Error Linear Units<br>
 For more details, see <i>Gaussian Error Linear Units (GELUs)</i> - <a href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a><br>
 This method uses the precise method<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="prelu-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-int...-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>prelu</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;prelu(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                      <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;alpha,
                      int...&nbsp;sharedAxes)</pre>
<div class="block">PReLU (Parameterized Rectified Linear Unit) operation.  Like LeakyReLU with a learnable alpha:<br>
 out[i] = in[i] if in[i] >= 0<br>
 out[i] = in[i] * alpha[i] otherwise<br>
 <br>
 sharedAxes allows you to share learnable parameters along axes.<br>
 For example, if the input has shape [batchSize, channels, height, width]<br>
 and you want each channel to have its own cutoff, use sharedAxes = [2, 3] and an<br>
 alpha with shape [channels].<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - Input data (NUMERIC type)</dd>
<dd><code>alpha</code> - The cutoff variable.  Note that the batch dimension (the 0th, whether it is batch or not) should not be part of alpha. (NUMERIC type)</dd>
<dd><code>sharedAxes</code> - Which axes to share cutoff parameters along. (Size: AtLeast(min=1))</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="relu-org.nd4j.linalg.api.ndarray.INDArray-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>relu</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;relu(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                     double&nbsp;cutoff)</pre>
<div class="block">Element-wise rectified linear function with specified cutoff:<br>
 out[i] = in[i] if in[i] >= cutoff<br>
 out[i] = 0 otherwise<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input (NUMERIC type)</dd>
<dd><code>cutoff</code> - Cutoff value for ReLU operation - x > cutoff ? x : 0. Usually 0</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="relu6-org.nd4j.linalg.api.ndarray.INDArray-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>relu6</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;relu6(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                      double&nbsp;cutoff)</pre>
<div class="block">Element-wise "rectified linear 6" function with specified cutoff:<br>
 out[i] = min(max(in, cutoff), 6)<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input (NUMERIC type)</dd>
<dd><code>cutoff</code> - Cutoff value for ReLU operation. Usually 0</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="reluLayer-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>reluLayer</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;reluLayer(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;input,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;weights,
                          <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;bias)</pre>
<div class="block">ReLU (Rectified Linear Unit) layer operation: out = relu(mmul(in,w) + bias)<br>
 Note that bias array is optional<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>input</code> - Input data (NUMERIC type)</dd>
<dd><code>weights</code> - Weights variable (NUMERIC type)</dd>
<dd><code>bias</code> - Optional bias variable (may be null) (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="selu-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>selu</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;selu(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise SeLU function - Scaled exponential Lineal Unit: see <a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a><br>
 <br>
 out[i] = scale * alpha * (exp(in[i])-1) if in[i]>0, or 0 if in[i] <= 0<br>
 Uses default scale and alpha values.<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="sigmoid-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sigmoid</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;sigmoid(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise sigmoid function: out[i] = 1.0/(1+exp(-in[i]))<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="sigmoidDerivative-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sigmoidDerivative</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;sigmoidDerivative(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                                  <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;wrt)</pre>
<div class="block">Element-wise sigmoid function derivative: dL/dIn given input and dL/dOut<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input Variable (NUMERIC type)</dd>
<dd><code>wrt</code> - Gradient at the output - dL/dOut. Must have same shape as the input (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output (gradient at input of sigmoid) (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="softmax-org.nd4j.linalg.api.ndarray.INDArray-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>softmax</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;softmax(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                        int&nbsp;dimension)</pre>
<div class="block">Softmax activation, along the specified dimension<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input (NUMERIC type)</dd>
<dd><code>dimension</code> - Dimension along which to apply softmax</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="softmax-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>softmax</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;softmax(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Softmax activation, along the specified dimension<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="softmaxDerivative-org.nd4j.linalg.api.ndarray.INDArray-org.nd4j.linalg.api.ndarray.INDArray-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>softmaxDerivative</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;softmaxDerivative(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x,
                                  <a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;wrt,
                                  int&nbsp;dimension)</pre>
<div class="block">Softmax derivative function<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Softmax input (NUMERIC type)</dd>
<dd><code>wrt</code> - Gradient at output, dL/dx (NUMERIC type)</dd>
<dd><code>dimension</code> - Softmax dimension</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output  (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="softplus-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>softplus</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;softplus(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise softplus function: out = log(exp(x) + 1)<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="softsign-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>softsign</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;softsign(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise softsign function: out = x / (abs(x) + 1)<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="softsignDerivative-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>softsignDerivative</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;softsignDerivative(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise derivative (dOut/dIn) of the softsign function softsign(INDArray)<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="swish-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>swish</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;swish(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Element-wise "swish" function: out = x * sigmoid(b*x) with b=1.0<br>
 See: <a href="https://arxiv.org/abs/1710.05941">https://arxiv.org/abs/1710.05941</a><br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
<a name="tanh-org.nd4j.linalg.api.ndarray.INDArray-">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>tanh</h4>
<pre>public&nbsp;<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;tanh(<a href="../../../../../org/nd4j/linalg/api/ndarray/INDArray.html" title="interface in org.nd4j.linalg.api.ndarray">INDArray</a>&nbsp;x)</pre>
<div class="block">Elementwise tanh (hyperbolic tangent) operation: out = tanh(x)<br></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>x</code> - Input variable (NUMERIC type)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>output Output variable (NUMERIC type)</dd>
</dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar.bottom">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.bottom" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.bottom.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="class-use/NDNN.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../../index-all.html">Index</a></li>
<li><a href="../../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../../org/nd4j/linalg/factory/ops/NDMath.html" title="class in org.nd4j.linalg.factory.ops"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../../../../org/nd4j/linalg/factory/ops/NDRandom.html" title="class in org.nd4j.linalg.factory.ops"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../../index.html?org/nd4j/linalg/factory/ops/NDNN.html" target="_top">Frames</a></li>
<li><a href="NDNN.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
<p class="legalCopy"><small>Copyright &#169; 2020. All rights reserved.</small></p>
</body>
</html>
