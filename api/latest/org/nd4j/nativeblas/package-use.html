<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="de">
<head>
<!-- Generated by javadoc (1.8.0_151) on Sat Mar 14 12:12:09 CET 2020 -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Uses of Package org.nd4j.nativeblas (deeplearning4j 1.0.0-beta6 API)</title>
<meta name="date" content="2020-03-14">
<link rel="stylesheet" type="text/css" href="../../../stylesheet.css" title="Style">
<script type="text/javascript" src="../../../script.js"></script>
</head>
<body>
<script type="text/javascript"><!--
    try {
        if (location.href.indexOf('is-external=true') == -1) {
            parent.document.title="Uses of Package org.nd4j.nativeblas (deeplearning4j 1.0.0-beta6 API)";
        }
    }
    catch(err) {
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar.top">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.top" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.top.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li>Class</li>
<li class="navBarCell1Rev">Use</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../index-all.html">Index</a></li>
<li><a href="../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li>Prev</li>
<li>Next</li>
</ul>
<ul class="navList">
<li><a href="../../../index.html?org/nd4j/nativeblas/package-use.html" target="_top">Frames</a></li>
<li><a href="package-use.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<a name="skip.navbar.top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<div class="header">
<h1 title="Uses of Package org.nd4j.nativeblas" class="title">Uses of Package<br>org.nd4j.nativeblas</h1>
</div>
<div class="contentContainer">
<ul class="blockList">
<li class="blockList">
<table class="useSummary" border="0" cellpadding="3" cellspacing="0" summary="Use table, listing packages, and an explanation">
<caption><span>Packages that use <a href="../../../org/nd4j/nativeblas/package-summary.html">org.nd4j.nativeblas</a></span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Package</th>
<th class="colLast" scope="col">Description</th>
</tr>
<tbody>
<tr class="altColor">
<td class="colFirst"><a href="#org.nd4j.linalg.cpu.nativecpu">org.nd4j.linalg.cpu.nativecpu</a></td>
<td class="colLast">&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><a href="#org.nd4j.linalg.cpu.nativecpu.blas">org.nd4j.linalg.cpu.nativecpu.blas</a></td>
<td class="colLast">&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><a href="#org.nd4j.nativeblas">org.nd4j.nativeblas</a></td>
<td class="colLast">&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><a href="#org.nd4j.rng">org.nd4j.rng</a></td>
<td class="colLast">&nbsp;</td>
</tr>
</tbody>
</table>
</li>
<li class="blockList"><a name="org.nd4j.linalg.cpu.nativecpu">
<!--   -->
</a>
<table class="useSummary" border="0" cellpadding="3" cellspacing="0" summary="Use table, listing classes, and an explanation">
<caption><span>Classes in <a href="../../../org/nd4j/nativeblas/package-summary.html">org.nd4j.nativeblas</a> used by <a href="../../../org/nd4j/linalg/cpu/nativecpu/package-summary.html">org.nd4j.linalg.cpu.nativecpu</a></span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Class and Description</th>
</tr>
<tbody>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/BaseNativeNDArrayFactory.html#org.nd4j.linalg.cpu.nativecpu">BaseNativeNDArrayFactory</a>
<div class="block">Base class with <a href="../../../org/nd4j/nativeblas/NativeOps.html" title="interface in org.nd4j.nativeblas"><code>NativeOps</code></a></div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/NativeOps.html#org.nd4j.linalg.cpu.nativecpu">NativeOps</a>
<div class="block">Native interface for
 op execution on cpu</div>
</td>
</tr>
</tbody>
</table>
</li>
<li class="blockList"><a name="org.nd4j.linalg.cpu.nativecpu.blas">
<!--   -->
</a>
<table class="useSummary" border="0" cellpadding="3" cellspacing="0" summary="Use table, listing classes, and an explanation">
<caption><span>Classes in <a href="../../../org/nd4j/nativeblas/package-summary.html">org.nd4j.nativeblas</a> used by <a href="../../../org/nd4j/linalg/cpu/nativecpu/blas/package-summary.html">org.nd4j.linalg.cpu.nativecpu.blas</a></span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Class and Description</th>
</tr>
<tbody>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jBlas.html#org.nd4j.linalg.cpu.nativecpu.blas">Nd4jBlas</a>
<div class="block">CBlas bindings

 Original credit:
 https://github.com/uncomplicate/neanderthal-atlas</div>
</td>
</tr>
</tbody>
</table>
</li>
<li class="blockList"><a name="org.nd4j.nativeblas">
<!--   -->
</a>
<table class="useSummary" border="0" cellpadding="3" cellspacing="0" summary="Use table, listing classes, and an explanation">
<caption><span>Classes in <a href="../../../org/nd4j/nativeblas/package-summary.html">org.nd4j.nativeblas</a> used by <a href="../../../org/nd4j/nativeblas/package-summary.html">org.nd4j.nativeblas</a></span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Class and Description</th>
</tr>
<tbody>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/NativeOps.html#org.nd4j.nativeblas">NativeOps</a>
<div class="block">Native interface for
 op execution on cpu</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/NativeOpsHolder.html#org.nd4j.nativeblas">NativeOpsHolder</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu._loader.html#org.nd4j.nativeblas">Nd4jCpu._loader</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.absolute_difference_loss.html#org.nd4j.nativeblas">Nd4jCpu.absolute_difference_loss</a>
<div class="block">Implementation of Absolute Difference loss function |predictions - labels|
 
 Input arrays: 
    0: predictions - the predicted values, type float.
    1: weights - is used for weighting (multiplying) of loss values, type float.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.absolute_difference_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.absolute_difference_loss_grad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.add.html#org.nd4j.nativeblas">Nd4jCpu.add</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.add_bp.html#org.nd4j.nativeblas">Nd4jCpu.add_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.adjust_contrast.html#org.nd4j.nativeblas">Nd4jCpu.adjust_contrast</a>
<div class="block">This operation adjusts image contrast by given factor ( z = (x - mean) * factor + mean )
 Input arrays:
 0 - input array with rank >= 3, must have last one dimension equal 3, that is dimension containing channels.
 1 - optional argument, input scalar-array containing saturation contrast factor

 T arguments:
 0 - optional argument, contrast factor</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.adjust_contrast_v2.html#org.nd4j.nativeblas">Nd4jCpu.adjust_contrast_v2</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.adjust_hue.html#org.nd4j.nativeblas">Nd4jCpu.adjust_hue</a>
<div class="block">This operation adjusts image hue by delta
 Input arrays:
 0 - input array with rank >= 3, must have at least one dimension equal 3, that is dimension containing channels.
 1 - optional argument, input scalar-array containing delta

 T arguments:
 0 - optional argument, delta value

 Int arguments:
 0 - optional argument, corresponds to dimension with 3 channels</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.adjust_saturation.html#org.nd4j.nativeblas">Nd4jCpu.adjust_saturation</a>
<div class="block">This operation adjusts image saturation by delta
 Input arrays:
 0 - input array with rank >= 3, must have at least one dimension equal 3, that is dimension containing channels.
 1 - optional argument, input scalar-array containing saturation factor

 T arguments:
 0 - optional argument, saturation factor

 Int arguments:
 0 - optional argument, corresponds to dimension with 3 channels</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.alpha_dropout_bp.html#org.nd4j.nativeblas">Nd4jCpu.alpha_dropout_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.apply_sgd.html#org.nd4j.nativeblas">Nd4jCpu.apply_sgd</a>
<div class="block">This operation updates parameters with provided gradients, wrt learning rate
 Expected arguments:
 x: parameters, any shape
 y: gradients. same shape as x
 lr: optional, learning rate

 T args:
 0: optional, learning rate</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.argmax.html#org.nd4j.nativeblas">Nd4jCpu.argmax</a>
<div class="block">This operation returns index of max element in a given NDArray (optionally: along given dimension(s))
 Expected input:
 0: N-dimensional array
 1: optional axis vector

 Int args:
 0: optional axis</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.argmin.html#org.nd4j.nativeblas">Nd4jCpu.argmin</a>
<div class="block">This operation returns index of min element in a given NDArray (optionally: along given dimension(s))
 Expected input:
 0: N-dimensional array
 1: optional axis vector

 Int args:
 0: optional axis</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ArgumentsList.html#org.nd4j.nativeblas">Nd4jCpu.ArgumentsList</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Assert.html#org.nd4j.nativeblas">Nd4jCpu.Assert</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.assign.html#org.nd4j.nativeblas">Nd4jCpu.assign</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.assign_bp.html#org.nd4j.nativeblas">Nd4jCpu.assign_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.avgpool2d.html#org.nd4j.nativeblas">Nd4jCpu.avgpool2d</a>
<div class="block">This op implements average pooling for convolution networks.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.avgpool2d_bp.html#org.nd4j.nativeblas">Nd4jCpu.avgpool2d_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.avgpool3dnew.html#org.nd4j.nativeblas">Nd4jCpu.avgpool3dnew</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.avgpool3dnew_bp.html#org.nd4j.nativeblas">Nd4jCpu.avgpool3dnew_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.axpy.html#org.nd4j.nativeblas">Nd4jCpu.axpy</a>
<div class="block">This op is simple implementation of BLAS AXPY method.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.batch_to_space.html#org.nd4j.nativeblas">Nd4jCpu.batch_to_space</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.batch_to_space_nd.html#org.nd4j.nativeblas">Nd4jCpu.batch_to_space_nd</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.batched_gemm.html#org.nd4j.nativeblas">Nd4jCpu.batched_gemm</a>
<div class="block">This operation implements batched matrix multiplication
 Expected arguments:
 alpha: vector of T
 beta: vector of T
 ...: A, B matrices sequentially. i.e: AAAAABBBBB
 
 Integer arguments:
 transA, transB, M, N, K, ldA, ldB, ldC - usual BLAS gemm arguments
 batchCount - number of operations in this batch
 
 PLEASE NOTE: M, N, K, ldA, ldB, ldC should be equal for all matrices within batch.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.batchnorm.html#org.nd4j.nativeblas">Nd4jCpu.batchnorm</a>
<div class="block">Batch normalization implementation.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.batchnorm_bp.html#org.nd4j.nativeblas">Nd4jCpu.batchnorm_bp</a>
<div class="block">back prop in batch normalization

 Expected arguments:
 input: input array (any number of dimensions)
 mean:
 variance:
 gamma: optional
 beta: optional
 dLdOut: next epsilon

 Int args:
 0: apply scale
 1: apply offset

 T args:
 0: epsilon

 output arrays:
 dL/dInput
 dL/dMean
 dL/dVariance
 dL/dGamma, optional
 dL/dBeta, optional</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.betainc.html#org.nd4j.nativeblas">Nd4jCpu.betainc</a>
<div class="block">This op calculates regularized incomplete beta integral Ix(a, b).</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.biasadd.html#org.nd4j.nativeblas">Nd4jCpu.biasadd</a>
<div class="block">This operation is added for compatibility purposes mostly.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.biasadd_bp.html#org.nd4j.nativeblas">Nd4jCpu.biasadd_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.bincount.html#org.nd4j.nativeblas">Nd4jCpu.bincount</a>
<div class="block">bincount operation return a vector with element counted.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.bitcast.html#org.nd4j.nativeblas">Nd4jCpu.bitcast</a>
<div class="block">This operation change type of input and modified shape of output to conform with given data type

 all as above op</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.bits_hamming_distance.html#org.nd4j.nativeblas">Nd4jCpu.bits_hamming_distance</a>
<div class="block">This operation returns hamming distance based on bits

 PLEASE NOTE: This operation is applicable only to integer data types

 \tparam T</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.bitwise_and.html#org.nd4j.nativeblas">Nd4jCpu.bitwise_and</a>
<div class="block">This operation applies bitwise AND

 PLEASE NOTE: This operation is applicable only to integer data types

 \tparam T</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.bitwise_or.html#org.nd4j.nativeblas">Nd4jCpu.bitwise_or</a>
<div class="block">This operation applies bitwise OR

 PLEASE NOTE: This operation is applicable only to integer data types

 \tparam T</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.bitwise_xor.html#org.nd4j.nativeblas">Nd4jCpu.bitwise_xor</a>
<div class="block">This operation applies bitwise XOR

 PLEASE NOTE: This operation is applicable only to integer data types

 \tparam T</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.boolean_and.html#org.nd4j.nativeblas">Nd4jCpu.boolean_and</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.boolean_not.html#org.nd4j.nativeblas">Nd4jCpu.boolean_not</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.boolean_or.html#org.nd4j.nativeblas">Nd4jCpu.boolean_or</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.boolean_xor.html#org.nd4j.nativeblas">Nd4jCpu.boolean_xor</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.BooleanOp.html#org.nd4j.nativeblas">Nd4jCpu.BooleanOp</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.broadcast_dynamic_shape.html#org.nd4j.nativeblas">Nd4jCpu.broadcast_dynamic_shape</a>
<div class="block">broadcast_dynamic_shape op.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.broadcast_to.html#org.nd4j.nativeblas">Nd4jCpu.broadcast_to</a>
<div class="block">This op broadcast given input up to given shape
  
 inputs:
  input array - array to be broadcasted to given shape
  shape array - array containing shape be broadcasted to</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.BroadcastableOp.html#org.nd4j.nativeblas">Nd4jCpu.BroadcastableOp</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.broadcastgradientargs.html#org.nd4j.nativeblas">Nd4jCpu.broadcastgradientargs</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cast.html#org.nd4j.nativeblas">Nd4jCpu.cast</a>
<div class="block">This operation casts elements of input array to specified data type
 
 PLEASE NOTE: This op is disabled atm, and reserved for future releases.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.check_numerics.html#org.nd4j.nativeblas">Nd4jCpu.check_numerics</a>
<div class="block">This op checks for Inf/NaN values within input array, and throws exception if there's at least one</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cholesky.html#org.nd4j.nativeblas">Nd4jCpu.cholesky</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.choose.html#org.nd4j.nativeblas">Nd4jCpu.choose</a>
<div class="block">This op takes either 1 argument and 1 scalar
 or 1 argument and another comparison array
 and runs a pre defined conditional op.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.clip_by_global_norm.html#org.nd4j.nativeblas">Nd4jCpu.clip_by_global_norm</a>
<div class="block">clip a list of given tensors with given average norm when needed

 Input:
    a list of tensors (at least one)

 Input floating point argument:
    clip_norm - a value that used as threshold value and norm to be used

 return a list of clipped tensors
  and global_norm as scalar tensor at the end</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.clipbyavgnorm.html#org.nd4j.nativeblas">Nd4jCpu.clipbyavgnorm</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.clipbynorm.html#org.nd4j.nativeblas">Nd4jCpu.clipbynorm</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.clipbynorm_bp.html#org.nd4j.nativeblas">Nd4jCpu.clipbynorm_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.clipbyvalue.html#org.nd4j.nativeblas">Nd4jCpu.clipbyvalue</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.clone_list.html#org.nd4j.nativeblas">Nd4jCpu.clone_list</a>
<div class="block">This operation clones given NDArrayList</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.col2im.html#org.nd4j.nativeblas">Nd4jCpu.col2im</a>
<div class="block">This op implements col2im algorithm, widely used in convolution neural networks
 Input: 6D input expected (like output of im2col op)

 Int args:
 0: stride height
 1: stride width
 2: padding height
 3: padding width
 4: image height
 5: image width
 6: dilation height
 7: dilation width</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.compare_and_bitpack.html#org.nd4j.nativeblas">Nd4jCpu.compare_and_bitpack</a>
<div class="block">compare_and_bitpack - compare with greater and pack result with uint8

 input params:
    0 - NDArray (input)
    1 - 0D Tensor - threshold


 output:
    0 - NDArray with the same shape as input and type uint8</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.concat.html#org.nd4j.nativeblas">Nd4jCpu.concat</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.concat_bp.html#org.nd4j.nativeblas">Nd4jCpu.concat_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Conditional.html#org.nd4j.nativeblas">Nd4jCpu.Conditional</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.confusion_matrix.html#org.nd4j.nativeblas">Nd4jCpu.confusion_matrix</a>
<div class="block">This operation calculate the confusion matrix for a
 pair of prediction and label 1-D arrays.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ConstantDataBuffer.html#org.nd4j.nativeblas">Nd4jCpu.ConstantDataBuffer</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ConstantDescriptor.html#org.nd4j.nativeblas">Nd4jCpu.ConstantDescriptor</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Context.html#org.nd4j.nativeblas">Nd4jCpu.Context</a>
<div class="block">This class defines input desired for any given node/operation within graph</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ContextBuffers.html#org.nd4j.nativeblas">Nd4jCpu.ContextBuffers</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ContextPrototype.html#org.nd4j.nativeblas">Nd4jCpu.ContextPrototype</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.conv1d.html#org.nd4j.nativeblas">Nd4jCpu.conv1d</a>
<div class="block">1D temporal convolution implementation
 Expected input:
 x: 3D array
 weight: 3D Array
 bias: optional vector

 Int args:
 0: kernel
 1: stride
 2: padding</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.conv1d_bp.html#org.nd4j.nativeblas">Nd4jCpu.conv1d_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.conv2d.html#org.nd4j.nativeblas">Nd4jCpu.conv2d</a>
<div class="block">2D convolution implementation
 Expected input:
 x: 4D array
 weight: 4D Array
 bias: optional vector, length of outputChannels

 IntArgs:
 0: kernel height
 1: kernel width
 2: stride height
 3: stride width
 4: padding height
 5: padding width
 6: dilation height
 7: dilation width
 8: same mode:   1 true, 0 false
 9: data format: 1 NHWC, 0 NCHW</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.conv2d_bp.html#org.nd4j.nativeblas">Nd4jCpu.conv2d_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.conv2d_input_bp.html#org.nd4j.nativeblas">Nd4jCpu.conv2d_input_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.conv3dnew.html#org.nd4j.nativeblas">Nd4jCpu.conv3dnew</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.conv3dnew_bp.html#org.nd4j.nativeblas">Nd4jCpu.conv3dnew_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cosine_distance_loss.html#org.nd4j.nativeblas">Nd4jCpu.cosine_distance_loss</a>
<div class="block">Implementation of cosine-distance loss function 1. - (predictions * labels).reduce_sum_along(dimension)
 
 Input arrays: 
    0: predictions - the predicted values, type float
    1: weights - is used for weighting (multiplying) of loss values, type float.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cosine_distance_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.cosine_distance_loss_grad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.create.html#org.nd4j.nativeblas">Nd4jCpu.create</a>
<div class="block">This operation creates new array
 Input:
    array with shape values

 IArgs:
    order value
    data type value

 BArgs:
    initialization option</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.create_list.html#org.nd4j.nativeblas">Nd4jCpu.create_list</a>
<div class="block">This operation creates new empty NDArrayList</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.crelu.html#org.nd4j.nativeblas">Nd4jCpu.crelu</a>
<div class="block">This is Concatenated RELU implementation.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.crelu_bp.html#org.nd4j.nativeblas">Nd4jCpu.crelu_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.crop_and_resize.html#org.nd4j.nativeblas">Nd4jCpu.crop_and_resize</a>
<div class="block">This op make bilinear or nearest neighbor interpolated resize for given tensor

 input array:
    0 - 4D-Tensor with shape (batch, sizeX, sizeY, channels) numeric type
    1 - 2D-Tensor with shape (num_boxes, 4) float type
    2 - 1D-Tensor with shape (num_boxes) int type
    3 - 1D-Tensor with 2 values (newWidth, newHeight) (optional) int type

 float arguments (optional)
   0 - exprapolation_value (optional) default 0.f

 int arguments: (optional)
   0 - mode (default 0 - bilinear interpolation)

 output array:
   the 4D-Tensor with resized to crop_size images given - float type</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cross.html#org.nd4j.nativeblas">Nd4jCpu.cross</a>
<div class="block">This op calculates cross-product between input arguments
 Input arguments
 0 - vector or tensor A
 1 - vector or tensor B</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cube.html#org.nd4j.nativeblas">Nd4jCpu.cube</a>
<div class="block">This is Cube activation function.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cube_bp.html#org.nd4j.nativeblas">Nd4jCpu.cube_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cumprod.html#org.nd4j.nativeblas">Nd4jCpu.cumprod</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cumprod_bp.html#org.nd4j.nativeblas">Nd4jCpu.cumprod_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cumsum.html#org.nd4j.nativeblas">Nd4jCpu.cumsum</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cumsum_bp.html#org.nd4j.nativeblas">Nd4jCpu.cumsum_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.CurrentIndexing.html#org.nd4j.nativeblas">Nd4jCpu.CurrentIndexing</a>
<div class="block">Indexing information
 for bounds checking</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cyclic_rshift_bits.html#org.nd4j.nativeblas">Nd4jCpu.cyclic_rshift_bits</a>
<div class="block">This operation shift individual bits of each element in array, shifting to the right

 PLEASE NOTE: This operation is applicable only to integer data types

 \tparam T</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.cyclic_shift_bits.html#org.nd4j.nativeblas">Nd4jCpu.cyclic_shift_bits</a>
<div class="block">This operation shift individual bits of each element in array, shifting to the left

 PLEASE NOTE: This operation is applicable only to integer data types

 \tparam T</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.DebugInfo.html#org.nd4j.nativeblas">Nd4jCpu.DebugInfo</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.DeclarableCustomOp.html#org.nd4j.nativeblas">Nd4jCpu.DeclarableCustomOp</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.DeclarableListOp.html#org.nd4j.nativeblas">Nd4jCpu.DeclarableListOp</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.DeclarableOp.html#org.nd4j.nativeblas">Nd4jCpu.DeclarableOp</a>
<div class="block">This class is the basic building block of Graph Operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.DeclarableReductionOp.html#org.nd4j.nativeblas">Nd4jCpu.DeclarableReductionOp</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.deconv2d.html#org.nd4j.nativeblas">Nd4jCpu.deconv2d</a>
<div class="block">2D deconvolution implementation

 IntArgs:
 0: kernel height
 1: kernel width
 2: stride height
 3: stride width
 4: padding height
 5: padding width
 6: dilation height
 7: dilation width
 8: same mode: 0 false, 1 true</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.deconv2d_bp.html#org.nd4j.nativeblas">Nd4jCpu.deconv2d_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.deconv2d_tf.html#org.nd4j.nativeblas">Nd4jCpu.deconv2d_tf</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.deconv3d.html#org.nd4j.nativeblas">Nd4jCpu.deconv3d</a>
<div class="block">3D deconvolution implementation

 IntArgs:
 0:  filter(kernel) depth
 1:  filter(kernel) height
 2:  filter(kernel) width
 3:  strides depth
 4:  strides height
 5:  strides width
 6:  paddings depth
 7:  paddings height
 8:  paddings width
 9:  dilations depth
 10: dilations height
 11: dilations width
 12: same mode: 0 false, 1 true
 13: data format (optional): 0-NDHWC, 1-NCDHW, default is 1</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.deconv3d_bp.html#org.nd4j.nativeblas">Nd4jCpu.deconv3d_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.depth_to_space.html#org.nd4j.nativeblas">Nd4jCpu.depth_to_space</a>
<div class="block">This operation rearranges data from depth into blocks of spatial data.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.depthwise_conv2d.html#org.nd4j.nativeblas">Nd4jCpu.depthwise_conv2d</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.depthwise_conv2d_bp.html#org.nd4j.nativeblas">Nd4jCpu.depthwise_conv2d_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.diag.html#org.nd4j.nativeblas">Nd4jCpu.diag</a>
<div class="block">Returns a diagonal tensor with a given diagonal values.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.diag_part.html#org.nd4j.nativeblas">Nd4jCpu.diag_part</a>
<div class="block">Returns a diagonal tensor with a given diagonal values.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.digamma.html#org.nd4j.nativeblas">Nd4jCpu.digamma</a>
<div class="block">This op calculates digamma function psi(x) = derivative of log(Gamma(x))

 Input arrays:
    0: x - abscissa points where to evaluate the digamma function, type float

 Output array:
    0: values of digamma function at corresponding x, type float</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dilation2d.html#org.nd4j.nativeblas">Nd4jCpu.dilation2d</a>
<div class="block">Dilation2D op

 Int args:
 0: isSameMode</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.divide.html#org.nd4j.nativeblas">Nd4jCpu.divide</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.divide_bp.html#org.nd4j.nativeblas">Nd4jCpu.divide_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.divide_no_nan.html#org.nd4j.nativeblas">Nd4jCpu.divide_no_nan</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dot_product_attention.html#org.nd4j.nativeblas">Nd4jCpu.dot_product_attention</a>
<div class="block">This operation performs dot product attention on the given timeseries input with the given queries
 out = sum(similarity(k_i, q) * v_i)

 similarity(k, q) = softmax(k * q) where x * q is the dot product of x and q

 Optionally with normalization step:
 similarity(k, q) = softmax(k * q / sqrt(size(q))

 See also "Attention is all you need" (https://arxiv.org/abs/1706.03762, p. 4, eq. 1)

 Note: This supports multiple queries at once, if only one query is available the queries vector still has to
 be 3D but can have queryCount = 1

 Note: keys and values usually is the same array.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dot_product_attention_bp.html#org.nd4j.nativeblas">Nd4jCpu.dot_product_attention_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.draw_bounding_boxes.html#org.nd4j.nativeblas">Nd4jCpu.draw_bounding_boxes</a>
<div class="block">draw_bounding_boxes op - modified input image with given colors exept given boxes.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dropout.html#org.nd4j.nativeblas">Nd4jCpu.dropout</a>
<div class="block">This op calculates dropout of input
 Input arguments
  0 - input tensor
  1 - noise_shape - (vector with shape to reduce) - optional

  int parameter - seed for random numbers
  T parameter - probability (should be between 0 and 1)
  return value - a tensor with the same shape as target or input</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dropout_bp.html#org.nd4j.nativeblas">Nd4jCpu.dropout_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dynamic_bidirectional_rnn.html#org.nd4j.nativeblas">Nd4jCpu.dynamic_bidirectional_rnn</a>
<div class="block">Implementation of operation "static RNN time sequences" with peep hole connections:

 Input arrays:
    0: input with shape [time x batchSize x inSize] or [batchSize x time x inSize], time - number of time steps, batchSize - batch size, inSize - number of features
    1: input-to-hidden  weights for forward RNN, [inSize   x numUnitsFW]
    2: hidden-to-hidden weights for forward RNN, [numUnitsFW x numUnitsFW]
    3: biases for forward RNN, [2*numUnitsFW]
    4: input-to-hidden  weights for backward RNN, [inSize   x numUnitsBW]
    5: hidden-to-hidden weights for backward RNN, [numUnitsBW x numUnitsBW]
    6: biases for backward RNN, [2*numUnitsBW]
    7: (optional) initial cell output for forward RNN [batchSize x numUnitsFW], that is at time step = 0
    8: (optional) initial cell output for backward RNN [batchSize x numUnitsBW], that is at time step = 0
    9: (optional) vector with shape [batchSize] containing integer values within [0,time), each element of this vector set max time step per each input in batch, this provides no calculations for time >= maxTimeStep

  Input integer arguments:
    0: (optional) timeMajor - if non zero then input shape is [time, batchSize, ...], else [batchSize, time, ...]</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dynamic_partition.html#org.nd4j.nativeblas">Nd4jCpu.dynamic_partition</a>
<div class="block">dynamic_partition - partition a input tensor onto num_partitions
 accordingly to index array given.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dynamic_partition_bp.html#org.nd4j.nativeblas">Nd4jCpu.dynamic_partition_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dynamic_rnn.html#org.nd4j.nativeblas">Nd4jCpu.dynamic_rnn</a>
<div class="block">Implementation of operation "static RNN time sequences" with peep hole connections:

 Input arrays:
    0: input with shape [time x batchSize x inSize] or [batchSize x time x numUnits], time - number of time steps, batchSize - batch size, inSize - number of features
    1: input-to-hidden  weights, [inSize   x numUnits]
    2: hidden-to-hidden weights, [numUnits x numUnits]
    3: biases, [2*numUnits]
    4: (optional) initial cell output [batchSize x numUnits], that is at time step = 0
    5: (optional) vector with shape [batchSize] containing integer values within [0,time), each element of this vector set max time step per each input in batch, this provides no calculations for time >= maxTimeStep

  Input integer arguments:
    0: (optional) timeMajor - if non zero then input shape is [time, batchSize, ...], else [batchSize, time, ...]</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.dynamic_stitch.html#org.nd4j.nativeblas">Nd4jCpu.dynamic_stitch</a>
<div class="block">dynamic_stitch - merge partitions from the second param a input tensor
 into a single tensor accordingly to index array given.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.elu.html#org.nd4j.nativeblas">Nd4jCpu.elu</a>
<div class="block">This op is ELU activation function.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.elu_bp.html#org.nd4j.nativeblas">Nd4jCpu.elu_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.embedding_lookup.html#org.nd4j.nativeblas">Nd4jCpu.embedding_lookup</a>
<div class="block">embedding_lookup - search for submatrices in given matrix and retunts them
 accordingly to index array given.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Environment.html#org.nd4j.nativeblas">Nd4jCpu.Environment</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.eq_scalar.html#org.nd4j.nativeblas">Nd4jCpu.eq_scalar</a>
<div class="block">This is scalar boolean op.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.equals.html#org.nd4j.nativeblas">Nd4jCpu.equals</a>
<div class="block">This op takes 2 equally shaped arrays as input, and provides binary matrix as output.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ErrorReference.html#org.nd4j.nativeblas">Nd4jCpu.ErrorReference</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.evaluate_reduction_shape.html#org.nd4j.nativeblas">Nd4jCpu.evaluate_reduction_shape</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.expand_dims.html#org.nd4j.nativeblas">Nd4jCpu.expand_dims</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.expose.html#org.nd4j.nativeblas">Nd4jCpu.expose</a>
<div class="block">This operations exposes given arguments as it's own outputs, but does it only once.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ExternalWorkspace.html#org.nd4j.nativeblas">Nd4jCpu.ExternalWorkspace</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.extract_image_patches.html#org.nd4j.nativeblas">Nd4jCpu.extract_image_patches</a>
<div class="block">extract_image_patches op - Extract patches from images and put them in the "depth" output dimension.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.eye.html#org.nd4j.nativeblas">Nd4jCpu.eye</a>
<div class="block">creates identity 2D matrix or batch of identical 2D identity matrices

 Input array:
 provide some array - in any case operation simply neglects it

 Input float argument (if passed):
 TArgs[0] - type of elements of output array, default value is 5 (float)

 Input integer arguments:
 IArgs[0]       - order of output identity matrix, 99 -> 'c'-order, 102 -> 'f'-order
 IArgs[1]       - the number of rows in output inner-most 2D identity matrix
 IArgs[2]       - optional, the number of columns in output inner-most 2D identity matrix, if this argument is not provided then it is taken to be equal to number of rows
 IArgs[3,4,...] - optional, shape of batch, output matrix will have leading batch dimensions of this shape</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.fake_quant_with_min_max_vars.html#org.nd4j.nativeblas">Nd4jCpu.fake_quant_with_min_max_vars</a>
<div class="block">fake_quant_with_min_max_vals - tf.quantization.fake_quant_with_min_max_vars

 input params:
    0 - NDArray (input)
    1 - 0D Tensor - min value
    2 - 0D Tensor - max value

 int params (optional):
    0 - num_bits (allowed interval [2, 16], default 8)
    1 - narrow_range (default False)

 output:
    0 - NDArray with the same shape as input</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.fake_quant_with_min_max_vars_per_channel.html#org.nd4j.nativeblas">Nd4jCpu.fake_quant_with_min_max_vars_per_channel</a>
<div class="block">fake_quant_with_min_max_vals_per_channel - tf.quantization.fake_quant_with_min_max_vars_per_channel

 input params:
    0 - NDArray (input) - at least 2D.
    1 - 1D Tensor - min values (min length equals to last dim of input)
    2 - 1D Tensor - max value (length equals to min)

 int params (optional):
    0 - num_bits (allowed interval [2, 16], default 8)
    1 - narrow_range (default False)

 output:
    0 - NDArray with the same shape as input</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.fill.html#org.nd4j.nativeblas">Nd4jCpu.fill</a>
<div class="block">This operation takes shape as first argument, and returns new NDArray filled with specific scalar value.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.fill_as.html#org.nd4j.nativeblas">Nd4jCpu.fill_as</a>
<div class="block">This operation takes input's shape, and returns new NDArray filled with specified value
 Expected arguments:
 input: N-dimensional array

 T args:
 0: scalar value, used to fill NDArray</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.firas_sparse.html#org.nd4j.nativeblas">Nd4jCpu.firas_sparse</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.flatten.html#org.nd4j.nativeblas">Nd4jCpu.flatten</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Floor.html#org.nd4j.nativeblas">Nd4jCpu.Floor</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.floordiv.html#org.nd4j.nativeblas">Nd4jCpu.floordiv</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.floordiv_bp.html#org.nd4j.nativeblas">Nd4jCpu.floordiv_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.floormod.html#org.nd4j.nativeblas">Nd4jCpu.floormod</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.floormod_bp.html#org.nd4j.nativeblas">Nd4jCpu.floormod_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.FlowPath.html#org.nd4j.nativeblas">Nd4jCpu.FlowPath</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.fused_batch_norm.html#org.nd4j.nativeblas">Nd4jCpu.fused_batch_norm</a>
<div class="block">This operation performs batch normalization of layer, it is based on following article https://arxiv.org/abs/1502.03167.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.gather.html#org.nd4j.nativeblas">Nd4jCpu.gather</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.gather_list.html#org.nd4j.nativeblas">Nd4jCpu.gather_list</a>
<div class="block">This operation builds NDArray from NDArrayList using indices
 Expected arguments:
 x: non-empty list
 indices: vector with indices for gather operation</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.gather_nd.html#org.nd4j.nativeblas">Nd4jCpu.gather_nd</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.get_seed.html#org.nd4j.nativeblas">Nd4jCpu.get_seed</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.GraphProfile.html#org.nd4j.nativeblas">Nd4jCpu.GraphProfile</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.greater.html#org.nd4j.nativeblas">Nd4jCpu.greater</a>
<div class="block">This op takes 2 equally shaped arrays as input, and provides binary matrix as output.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.greater_equal.html#org.nd4j.nativeblas">Nd4jCpu.greater_equal</a>
<div class="block">This op takes 2 equally shaped arrays as input, and provides binary matrix as output.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.gru.html#org.nd4j.nativeblas">Nd4jCpu.gru</a>
<div class="block">Implementation of gated Recurrent Unit:

 Input arrays:
    0: input with shape [time x batchSize x inSize], time - number of time steps, batchSize - batch size, inSize - number of features
    1: initial cell output [batchSize x numUnits],  that is at time step = 0
    2: input-to-hidden  weights, [inSize   x 3*numUnits]
    3: hidden-to-hidden weights, [numUnits x 3*numUnits]
    4: biases, [3*numUnits]

 Output arrays:
    0: cell outputs [time x batchSize x numUnits], that is per each time step</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.gruCell.html#org.nd4j.nativeblas">Nd4jCpu.gruCell</a>
<div class="block">Implementation of gated Recurrent Unit cell:
    Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio
    "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"

 Input arrays:
    0: input with shape [batchSize x inSize], batchSize - batch size, inSize - number of features
    1: previous cell output [batchSize x numUnits],  that is at previous time step t-1
    2: RU weights - [(inSize+numUnits), 2*numUnits] - reset and update gates (input/recurrent weights)
    3: C weights - [(inSize+numUnits), numUnits] - cell gate (input/recurrent weights)
    4: reset and update biases, [2*numUnits] - reset and update gates
    5: cell biases, [numUnits]

 Output arrays:
    0: Reset gate output [bS, numUnits]
    1: Update gate output [bS, numUnits]
    2: Cell gate output [bS, numUnits]
    3: Current cell output [bS, numUnits]</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.gruCell_bp.html#org.nd4j.nativeblas">Nd4jCpu.gruCell_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.gt_scalar.html#org.nd4j.nativeblas">Nd4jCpu.gt_scalar</a>
<div class="block">This is scalar boolean op.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.gte_scalar.html#org.nd4j.nativeblas">Nd4jCpu.gte_scalar</a>
<div class="block">This is scalar boolean op.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.hardsigmoid.html#org.nd4j.nativeblas">Nd4jCpu.hardsigmoid</a>
<div class="block">This is HardSigmoid activation function.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.hardsigmoid_bp.html#org.nd4j.nativeblas">Nd4jCpu.hardsigmoid_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.hardtanh.html#org.nd4j.nativeblas">Nd4jCpu.hardtanh</a>
<div class="block">This is HardTanh activation function.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.hardtanh_bp.html#org.nd4j.nativeblas">Nd4jCpu.hardtanh_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.hashcode.html#org.nd4j.nativeblas">Nd4jCpu.hashcode</a>
<div class="block">This operation calculates hash code, optionally along dimension</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.hinge_loss.html#org.nd4j.nativeblas">Nd4jCpu.hinge_loss</a>
<div class="block">Implementation of hinge loss function max(0, 1 - labels*logits)
 
 Input arrays: 
    0: logits - logits, type float
    1: weights - is used for weighting (multiplying) of loss values, type float.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.hinge_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.hinge_loss_grad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.histogram.html#org.nd4j.nativeblas">Nd4jCpu.histogram</a>
<div class="block">This operation calculates number of entries per bin</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.histogram_fixed_width.html#org.nd4j.nativeblas">Nd4jCpu.histogram_fixed_width</a>
<div class="block">returns histogram (as 1D array) with fixed bins width

 Input arrays:
 - input array with elements to be binned into output histogram
 - range array with first element being bottom limit and second element being top limit of histogram,
             please note that input_value <= range[0] will be mapped to histogram[0], input_value >= range[1] will be mapped to histogram[-1]

 Input integer arguments:
    nbins (optional) - number of histogram bins, default value is 100</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.huber_loss.html#org.nd4j.nativeblas">Nd4jCpu.huber_loss</a>
<div class="block">Implementation of Huber loss function:
    0.5 * (labels-predictions)^2                                if |labels-predictions| <= delta
    0.5 * delta^2 + delta * (|labels-predictions| - delta)      if |labels-predictions| >  delta
 
 Input arrays: 
    0: predictions - the predicted values, type float
    1: weights - is used for weighting (multiplying) of loss values, type float.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.huber_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.huber_loss_grad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.identity.html#org.nd4j.nativeblas">Nd4jCpu.identity</a>
<div class="block">This is Indentity operation.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.identity_bp.html#org.nd4j.nativeblas">Nd4jCpu.identity_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.identity_n.html#org.nd4j.nativeblas">Nd4jCpu.identity_n</a>
<div class="block">This is Indentity operation.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.igamma.html#org.nd4j.nativeblas">Nd4jCpu.igamma</a>
<div class="block">Broadcastable igamma implementation

 igamma(a, x) = gamma(а, x) / Gamma(a) - Gamma distribution function P(a,x)
 Gamma(a) = int from 0 to infinity { t ^ {a - 1} e^{-t}dt }
 gamma(a, x) = int from 0 to x { t ^ {a - 1} e^{-t}dt }
 \tparam T</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.igammac.html#org.nd4j.nativeblas">Nd4jCpu.igammac</a>
<div class="block">Broadcastable igammac implementation
 igammac(a, x) = Gamma(a,x)/Gamma(а) - Gamma distribution function Q(a,x)
 Gamma(a) = int from 0 to infinity { t ^ {a - 1} e^{-t}dt }
 Gamma(a, x) = int from x to infinity { t ^ {a - 1} e^{-t}dt }
 \tparam T</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.IGenerator.html#org.nd4j.nativeblas">Nd4jCpu.IGenerator</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.im2col.html#org.nd4j.nativeblas">Nd4jCpu.im2col</a>
<div class="block">This op implements im2col algorithm, widely used in convolution neural networks
 Input: 4D input expected

 Int args:
 0: kernel height
 1: kernel width
 2: stride height
 3: stride width
 4: padding height
 5: padding width
 6: dilation height
 7: dilation width
 8: isSameMode</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.im2col_bp.html#org.nd4j.nativeblas">Nd4jCpu.im2col_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.image_resize.html#org.nd4j.nativeblas">Nd4jCpu.image_resize</a>
<div class="block">This op make interpolated resize for given tensor with given algorithm.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.in_top_k.html#org.nd4j.nativeblas">Nd4jCpu.in_top_k</a>
<div class="block">in_top_k operation returns a vector of k boolean values for
  given NDArray as 2D matrix of predicted in the NDArray k top values
  The first parameter is a NDArray of predicted values (2d array).</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Intervals.html#org.nd4j.nativeblas">Nd4jCpu.Intervals</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.IntIntPair.html#org.nd4j.nativeblas">Nd4jCpu.IntIntPair</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.IntVectorVector.html#org.nd4j.nativeblas">Nd4jCpu.IntVectorVector</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.invert_permutation.html#org.nd4j.nativeblas">Nd4jCpu.invert_permutation</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.is_non_decreasing.html#org.nd4j.nativeblas">Nd4jCpu.is_non_decreasing</a>
<div class="block">This op takes 1 n-dimensional array as input, and returns true if for every adjacent pair we have x[i] <= x[i+1].</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.is_numeric_tensor.html#org.nd4j.nativeblas">Nd4jCpu.is_numeric_tensor</a>
<div class="block">This op takes 1 n-dimensional array as input, and returns true if input is a numeric array.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.is_strictly_increasing.html#org.nd4j.nativeblas">Nd4jCpu.is_strictly_increasing</a>
<div class="block">This op takes 1 n-dimensional array as input, and returns true if for every adjacent pair we have x[i] < x[i+1].</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ismax.html#org.nd4j.nativeblas">Nd4jCpu.ismax</a>
<div class="block">This op produces binary matrix wrt to target dimension.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.KeyPair.html#org.nd4j.nativeblas">Nd4jCpu.KeyPair</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.l2_loss.html#org.nd4j.nativeblas">Nd4jCpu.l2_loss</a>
<div class="block">l2_loss op.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.LaunchContext.html#org.nd4j.nativeblas">Nd4jCpu.LaunchContext</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.layer_norm.html#org.nd4j.nativeblas">Nd4jCpu.layer_norm</a>
<div class="block">applies layer normalization to input
 y = g * standardize(x) + b

 see nd4j::ops::standardize</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.layer_norm_bp.html#org.nd4j.nativeblas">Nd4jCpu.layer_norm_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.less.html#org.nd4j.nativeblas">Nd4jCpu.less</a>
<div class="block">This op takes 2 equally shaped arrays as input, and provides binary matrix as output.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.less_equal.html#org.nd4j.nativeblas">Nd4jCpu.less_equal</a>
<div class="block">This op takes 2 equally shaped arrays as input, and provides binary matrix as output.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lin_space.html#org.nd4j.nativeblas">Nd4jCpu.lin_space</a>
<div class="block">lin_space - op porting from TF (https://www.tensorflow.org/api_docs/python/tf/lin_space)

 input params:
    0 - startVal - NDArray scalar (float point)
    1 - finishVal - NDArray scalar (float point)
    2 - numOfElements - NDArray scalar (integer)

 output:
    0 - 1D NDArray with the same type as input and length as given with numOfElements param.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.listdiff.html#org.nd4j.nativeblas">Nd4jCpu.listdiff</a>
<div class="block">This operation takes 2 arrays: original values, and values to be excluded.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.log_loss.html#org.nd4j.nativeblas">Nd4jCpu.log_loss</a>
<div class="block">Implementation of logarithmic loss function ( y_i * log(p_i) + (1 - y_i) * log(1 - p_i) )
 
 Input arrays: 
    0: predictions - the predicted values, type float
    1: weights - is used for weighting (multiplying) of loss values, type float.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.log_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.log_loss_grad</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.log_matrix_determinant.html#org.nd4j.nativeblas">Nd4jCpu.log_matrix_determinant</a>
<div class="block">log_matrix_determinant op.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.log_poisson_loss.html#org.nd4j.nativeblas">Nd4jCpu.log_poisson_loss</a>
<div class="block">This op calculates logarithmic loss of poisson distributed input.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.log_poisson_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.log_poisson_loss_grad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.log_softmax.html#org.nd4j.nativeblas">Nd4jCpu.log_softmax</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.log_softmax_bp.html#org.nd4j.nativeblas">Nd4jCpu.log_softmax_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Log1p.html#org.nd4j.nativeblas">Nd4jCpu.Log1p</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.logdet.html#org.nd4j.nativeblas">Nd4jCpu.logdet</a>
<div class="block">logdet op.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.LogicOp.html#org.nd4j.nativeblas">Nd4jCpu.LogicOp</a>
<div class="block">Logic ops are unique snowflakes in any Graph.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.LongVectorVector.html#org.nd4j.nativeblas">Nd4jCpu.LongVectorVector</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lrelu.html#org.nd4j.nativeblas">Nd4jCpu.lrelu</a>
<div class="block">This is Leaky RELU activation function.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lrelu_bp.html#org.nd4j.nativeblas">Nd4jCpu.lrelu_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lrn.html#org.nd4j.nativeblas">Nd4jCpu.lrn</a>
<div class="block">Local response normalization implementation as TF.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lrn_bp.html#org.nd4j.nativeblas">Nd4jCpu.lrn_bp</a>
<div class="block">Local response normalization - backprop variant.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lstm.html#org.nd4j.nativeblas">Nd4jCpu.lstm</a>
<div class="block">Implementation of operation "LSTM time sequences" with peep hole connections:

 Input arrays:
    0: input with shape [time x batchSize x inSize], time - number of time steps, batchSize - batch size, inSize - number of features
    1: initial cell output [batchSize x numProj],  that is at time step = 0, in case of projection=false -> numProj=numUnits!!!</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lstmBlock.html#org.nd4j.nativeblas">Nd4jCpu.lstmBlock</a>
<div class="block">Implementation of operation for LSTM layer with optional peep hole connections.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lstmBlockCell.html#org.nd4j.nativeblas">Nd4jCpu.lstmBlockCell</a>
<div class="block">Implementation of operation for LSTM cell with optional peep hole connections:
    S.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lstmCell.html#org.nd4j.nativeblas">Nd4jCpu.lstmCell</a>
<div class="block">Implementation of operation for LSTM cell with peep hole connections:
    S.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lstmLayer.html#org.nd4j.nativeblas">Nd4jCpu.lstmLayer</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lt_scalar.html#org.nd4j.nativeblas">Nd4jCpu.lt_scalar</a>
<div class="block">This is scalar boolean op.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.lte_scalar.html#org.nd4j.nativeblas">Nd4jCpu.lte_scalar</a>
<div class="block">This is scalar boolean op.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.matmul.html#org.nd4j.nativeblas">Nd4jCpu.matmul</a>
<div class="block">This op is general matmum implementation.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.matmul_bp.html#org.nd4j.nativeblas">Nd4jCpu.matmul_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.matrix_band_part.html#org.nd4j.nativeblas">Nd4jCpu.matrix_band_part</a>
<div class="block">Copy a tensor setting everything outside a central band in each innermost matrix

 input array:
    x: given tensor with shape {..., M, N} - as vector (matrix) of matricies MxN

 int arguments:
   lower band
   upper band

 output array:
   matrix with given bands between lower and upper diagonals</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.matrix_determinant.html#org.nd4j.nativeblas">Nd4jCpu.matrix_determinant</a>
<div class="block">matrix_determinant op.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.matrix_diag.html#org.nd4j.nativeblas">Nd4jCpu.matrix_diag</a>
<div class="block">Inserts elements provided by diagonal array into the main diagonal of innermost matrices of output array,
 rest output elements are set to zeros

 Input array:
    diagonal: array containing elements to be inserted into output array,
              following rank condition is present: diagonal_rank = ouput_rank - 1

 Output array:
   0: is considered as batch of matrices, if for example diagonal array has shape [A,B,C] then output array has shape [A,B,C,C]</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.matrix_diag_part.html#org.nd4j.nativeblas">Nd4jCpu.matrix_diag_part</a>
<div class="block">Returns a diagonal vector for any submatricies with in a given tensor.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.matrix_inverse.html#org.nd4j.nativeblas">Nd4jCpu.matrix_inverse</a>
<div class="block">matrix_inverse op. - make inverse for all 2D square matricies found in the input tensor

 input params:
    0 - the tensor with dimension (x * y * z * ::: * M * M)

 return value:
    tensor with dimension (x * y * z * ::: * M * M) with inverse M x M matricies in it</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.matrix_set_diag.html#org.nd4j.nativeblas">Nd4jCpu.matrix_set_diag</a>
<div class="block">Inserts elements provided by diagonal array into the main diagonal of innermost matrices of input array

 Input arrays:
  0: input array, considered as batch of matrices
  1: diagonal array containing elements to be inserted into input array,
     following rank condition should be satisfied: diagonal_rank = input_rank - 1,
     the shapes of diagonal and input arrays must be equal except last dimension of input array,
     for example if input_shape = [A,B,C,D] then diagonal_shape = [A,B,C],
     also last dimension of diagonal array should be equal to smaller of last and last but one input dimensions
     that is: diagonal_shape[-1] = min(input_shape[-1], input_shape[-2])

 Output array:
  0: has the same shape as input, corresponding diagonal elements are substituted</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.max_pool_with_argmax.html#org.nd4j.nativeblas">Nd4jCpu.max_pool_with_argmax</a>
<div class="block">This op same as maxpool2d with a variant to return a matrix of indexes for max values

 Input - 4D tensor
 Output:
     0 - 4D tensor as input
     1 - 4D tensor with max value indexes

 Int params:
   9 int with 2x4 vectors and 1 bool value</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.maximum.html#org.nd4j.nativeblas">Nd4jCpu.maximum</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.maximum_bp.html#org.nd4j.nativeblas">Nd4jCpu.maximum_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.maxpool2d.html#org.nd4j.nativeblas">Nd4jCpu.maxpool2d</a>
<div class="block">This op implements max pooling for convolution networks.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.maxpool2d_bp.html#org.nd4j.nativeblas">Nd4jCpu.maxpool2d_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.maxpool3dnew.html#org.nd4j.nativeblas">Nd4jCpu.maxpool3dnew</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.maxpool3dnew_bp.html#org.nd4j.nativeblas">Nd4jCpu.maxpool3dnew_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mean_pairwssqerr_loss.html#org.nd4j.nativeblas">Nd4jCpu.mean_pairwssqerr_loss</a>
<div class="block">Implementation of pairwise-errors-squared loss function 
 
 Input arrays: 
    0: predictions - the predicted values, type float.
    1: weights - is used for weighting (multiplying) of loss values, type float.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mean_pairwssqerr_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.mean_pairwssqerr_loss_grad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mean_sqerr_loss.html#org.nd4j.nativeblas">Nd4jCpu.mean_sqerr_loss</a>
<div class="block">Implementation of Sum-of-Squares loss function 1/N * sum_{i}^{N}(predictions_i - labels_i)^2
 
 Input arrays: 
    0: predictions - the predicted values, type float
    1: weights - is used for weighting (multiplying) of loss values, type float.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mean_sqerr_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.mean_sqerr_loss_grad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mergeadd.html#org.nd4j.nativeblas">Nd4jCpu.mergeadd</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mergeavg.html#org.nd4j.nativeblas">Nd4jCpu.mergeavg</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mergemax.html#org.nd4j.nativeblas">Nd4jCpu.mergemax</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mergemaxindex.html#org.nd4j.nativeblas">Nd4jCpu.mergemaxindex</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.meshgrid.html#org.nd4j.nativeblas">Nd4jCpu.meshgrid</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.minimum.html#org.nd4j.nativeblas">Nd4jCpu.minimum</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.minimum_bp.html#org.nd4j.nativeblas">Nd4jCpu.minimum_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mirror_pad.html#org.nd4j.nativeblas">Nd4jCpu.mirror_pad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mod.html#org.nd4j.nativeblas">Nd4jCpu.mod</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.mod_bp.html#org.nd4j.nativeblas">Nd4jCpu.mod_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.moments.html#org.nd4j.nativeblas">Nd4jCpu.moments</a>
<div class="block">moments operation calculate a mean and variation for given NDArray
 with reduce a result according to axis array given.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.multi_head_dot_product_attention.html#org.nd4j.nativeblas">Nd4jCpu.multi_head_dot_product_attention</a>
<div class="block">This performs multi-headed dot product attention on the given timeseries input
 out = concat(head_1, head_2, ..., head_n) * Wo
 head_i = dot_product_attention(Wq_i*q, Wk_i*k, Wv_i*v)

 Optionally with normalization when calculating the attention for each head.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.multi_head_dot_product_attention_bp.html#org.nd4j.nativeblas">Nd4jCpu.multi_head_dot_product_attention_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.multiply.html#org.nd4j.nativeblas">Nd4jCpu.multiply</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.multiply_bp.html#org.nd4j.nativeblas">Nd4jCpu.multiply_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.NDArray.html#org.nd4j.nativeblas">Nd4jCpu.NDArray</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.NDArrayList.html#org.nd4j.nativeblas">Nd4jCpu.NDArrayList</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.NDArrayVector.html#org.nd4j.nativeblas">Nd4jCpu.NDArrayVector</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.NDArrayVector.Iterator.html#org.nd4j.nativeblas">Nd4jCpu.NDArrayVector.Iterator</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.NDIndex.html#org.nd4j.nativeblas">Nd4jCpu.NDIndex</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.NDIndexAll.html#org.nd4j.nativeblas">Nd4jCpu.NDIndexAll</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.neq_scalar.html#org.nd4j.nativeblas">Nd4jCpu.neq_scalar</a>
<div class="block">This is scalar boolean op.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.NodeProfile.html#org.nd4j.nativeblas">Nd4jCpu.NodeProfile</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.non_max_suppression.html#org.nd4j.nativeblas">Nd4jCpu.non_max_suppression</a>
<div class="block">image.non_max_suppression ops.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.non_max_suppression_overlaps.html#org.nd4j.nativeblas">Nd4jCpu.non_max_suppression_overlaps</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.non_max_suppression_v3.html#org.nd4j.nativeblas">Nd4jCpu.non_max_suppression_v3</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.noop.html#org.nd4j.nativeblas">Nd4jCpu.noop</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.norm.html#org.nd4j.nativeblas">Nd4jCpu.norm</a>
<div class="block">This operation provides various normalization modes:
 0: frobenius
 1: euclidean (norm2)
 2: norm1
 3: norm2
 4: inf-norm
 5: p-norm

 Expected arguments:
 input: N-dimensional array


 Int args:
 0...: axis

 T args:
 0: norm mode
 1: p for p-norm</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.normalize_moments.html#org.nd4j.nativeblas">Nd4jCpu.normalize_moments</a>
<div class="block">normalize_moments operation normalize already calculated mean and variation
 accordingly to shift and count.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.not_equals.html#org.nd4j.nativeblas">Nd4jCpu.not_equals</a>
<div class="block">This op takes 2 equally shaped arrays as input, and provides binary matrix as output.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.nth_element.html#org.nd4j.nativeblas">Nd4jCpu.nth_element</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.onehot.html#org.nd4j.nativeblas">Nd4jCpu.onehot</a>
<div class="block">This operation return one-hot encoded n-dimensional array
 Expected arguments:
 input: N-dimensional array

 T args:
 0: 'on' value
 1: 'off' value

 Int args:
 0: depth
 1: axis</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ones_as.html#org.nd4j.nativeblas">Nd4jCpu.ones_as</a>
<div class="block">This operation takes input's shape, and returns new NDArray filled with ones
 Expected arguments:
 input: N-dimensional array</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.OpArgsHolder.html#org.nd4j.nativeblas">Nd4jCpu.OpArgsHolder</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.OpDescriptor.html#org.nd4j.nativeblas">Nd4jCpu.OpDescriptor</a>
<div class="block">This class is very basic info holder for ops. bean/pojo pretty much.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.OpRegistrator.html#org.nd4j.nativeblas">Nd4jCpu.OpRegistrator</a>
<div class="block">This class provides runtime ops lookup, based on opName or opHash.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.order.html#org.nd4j.nativeblas">Nd4jCpu.order</a>
<div class="block">This op changes order of given array to specified order.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.pad.html#org.nd4j.nativeblas">Nd4jCpu.pad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Pair.html#org.nd4j.nativeblas">Nd4jCpu.Pair</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.parallel_stack.html#org.nd4j.nativeblas">Nd4jCpu.parallel_stack</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.percentile.html#org.nd4j.nativeblas">Nd4jCpu.percentile</a>
<div class="block">This operation performs calculation of percentile of input array along given axises

 Input - tensor with rank N > 0
 Output - tensor with rank (N - length(axis)) or scalar if number of Integer arguments is zero
 Float arguments:
   0: percentile (scalar) in range [0,100] (inclusively)
   1: interpolation (optional), possible values are 0-"lower", 1-"higher", 2-"nearest"(default)
   2: keepDims (optional), if it is non zero, then unities are kept in reduced resulting shape of output array, default is 0
 Integer arguments - axis - the sequence of axises to calculate percentile along, if sequence is empty then calculate percentile for whole input tensor and return result as scalar</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.permute.html#org.nd4j.nativeblas">Nd4jCpu.permute</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.pick_list.html#org.nd4j.nativeblas">Nd4jCpu.pick_list</a>
<div class="block">This operations selects specified indices fron NDArrayList and returns them as NDArray
 Expected arguments:
 x: non-empty list
 indices: optional, vector with indices
 
 Int args:
 optional, indices</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.PlatformHelper.html#org.nd4j.nativeblas">Nd4jCpu.PlatformHelper</a>
<div class="block">This abstract class defines methods used by platform-specific helpers implementations</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.pnormpool2d.html#org.nd4j.nativeblas">Nd4jCpu.pnormpool2d</a>
<div class="block">This op implements pnorm pooling for convolution networks.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.pnormpool2d_bp.html#org.nd4j.nativeblas">Nd4jCpu.pnormpool2d_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.pointwise_conv2d.html#org.nd4j.nativeblas">Nd4jCpu.pointwise_conv2d</a>
<div class="block">point-wise 2D convolution
 Expected input:
 x: 4D array
 weight: 4D Array [1,  1,  iC, oC] (NHWC) or [oC, iC,  1,  1] (NCHW)
 bias: optional vector, length of oC

 IntArgs:
 0: data format: 1 NHWC, 0 NCHW (optional, by default = NHWC)</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.polygamma.html#org.nd4j.nativeblas">Nd4jCpu.polygamma</a>
<div class="block">This op calculates polygamma function psi^(n)(x).</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Pow.html#org.nd4j.nativeblas">Nd4jCpu.Pow</a>
<div class="block">Broadcastable pow implementation
 \tparam T</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.prelu.html#org.nd4j.nativeblas">Nd4jCpu.prelu</a>
<div class="block">Parametric Rectified Linear Unit
 f(x) = alpha * x for x < 0, f(x) = x for x >= 0</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.prelu_bp.html#org.nd4j.nativeblas">Nd4jCpu.prelu_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.random_bernoulli.html#org.nd4j.nativeblas">Nd4jCpu.random_bernoulli</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.random_crop.html#org.nd4j.nativeblas">Nd4jCpu.random_crop</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.random_exponential.html#org.nd4j.nativeblas">Nd4jCpu.random_exponential</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.random_gamma.html#org.nd4j.nativeblas">Nd4jCpu.random_gamma</a>
<div class="block">random_gamma op.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.random_normal.html#org.nd4j.nativeblas">Nd4jCpu.random_normal</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.random_poisson.html#org.nd4j.nativeblas">Nd4jCpu.random_poisson</a>
<div class="block">random_poisson op.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.random_shuffle.html#org.nd4j.nativeblas">Nd4jCpu.random_shuffle</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.RandomBuffer.html#org.nd4j.nativeblas">Nd4jCpu.RandomBuffer</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.RandomGenerator.html#org.nd4j.nativeblas">Nd4jCpu.RandomGenerator</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.randomuniform.html#org.nd4j.nativeblas">Nd4jCpu.randomuniform</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.range.html#org.nd4j.nativeblas">Nd4jCpu.range</a>
<div class="block">This operation generate sequences.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.rank.html#org.nd4j.nativeblas">Nd4jCpu.rank</a>
<div class="block">This operation returns rank of input array as scalar value.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.rationaltanh.html#org.nd4j.nativeblas">Nd4jCpu.rationaltanh</a>
<div class="block">This is RationalTanh activation function.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.rationaltanh_bp.html#org.nd4j.nativeblas">Nd4jCpu.rationaltanh_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.read_list.html#org.nd4j.nativeblas">Nd4jCpu.read_list</a>
<div class="block">This operations selects specified index fron NDArrayList and returns it as NDArray
 Expected arguments:
 x: non-empty list
 indices: optional, scalar with index
 
 Int args:
 optional, index</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.realdiv.html#org.nd4j.nativeblas">Nd4jCpu.realdiv</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.realdiv_bp.html#org.nd4j.nativeblas">Nd4jCpu.realdiv_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.rectifiedtanh.html#org.nd4j.nativeblas">Nd4jCpu.rectifiedtanh</a>
<div class="block">This is RectifiedTanh activation function.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.rectifiedtanh_bp.html#org.nd4j.nativeblas">Nd4jCpu.rectifiedtanh_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_dot_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_dot_bp</a>
<div class="block">This op calculates backprop dot for two tensors along given dimensions

 input array:
    x: tensor to calculate dot for
    y: tensor to calculate dot for
    z: tensor with gradient output of the FF dot for x and y

 int arguments:
   list of integers - dimensions to calculate dot along,
   default corresponds to empty list in which case calculation
   is performed for all dimensions and scalar is returned.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_logsumexp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_logsumexp</a>
<div class="block">reduce_logsumexp - tf.reduce_logsumexe operation

 input params:
    0 - NDArray (input)
    1 - 1D NDArray (axis) (optional) - integer array

 T_ARG param (optional):
 0 - keep_dims !</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_max.html#org.nd4j.nativeblas">Nd4jCpu.reduce_max</a>
<div class="block">This op calculates max of elements along given dimensions

 input array:
    x: tensor to calculate maxes for

 float arguments:
   keepDims: if non zero, then keep reduced dimensions with length = 1, default value is zero

 int arguments:
    list of integers - dimensions to calculate max along, default corresponds to empty list in which case calculation is performed for all dimensions and scalar is returned

 output array:
    reduced tensor with calculated maxes</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_max_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_max_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_mean.html#org.nd4j.nativeblas">Nd4jCpu.reduce_mean</a>
<div class="block">This op calculates mean of elements along given dimensions

 input array:
    x: tensor to calculate mean for

 float arguments:
   keepDims: if non zero, then keep reduced dimensions with length = 1, default value is zero

 int arguments:
    list of integers - dimensions to calculate mean along, default corresponds to empty list in which case calculation is performed for all dimensions and scalar is returned

 output array:
    reduced tensor with calculated means</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_mean_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_mean_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_min.html#org.nd4j.nativeblas">Nd4jCpu.reduce_min</a>
<div class="block">This op calculates min of elements along given dimensions

 input array:
    x: tensor to calculate mins for

 float arguments:
   keepDims: if non zero, then keep reduced dimensions with length = 1, default value is zero

 int arguments:
    list of integers - dimensions to calculate min along, default corresponds to empty list in which case calculation is performed for all dimensions and scalar is returned

 output array:
    reduced tensor with calculated mins</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_min_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_min_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_norm_max.html#org.nd4j.nativeblas">Nd4jCpu.reduce_norm_max</a>
<div class="block">This op calculates norm max of elements along given dimensions

 input array:
    x: tensor to calculate norm max for

 float arguments:
   keepDims: if non zero, then keep reduced dimensions with length = 1, default value is zero

 int arguments:
    list of integers - dimensions to calculate norm max along, default corresponds to empty list in which case calculation is performed for all dimensions and scalar is returned

 output array:
    reduced tensor with calculated norm</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_norm_max_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_norm_max_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_norm1.html#org.nd4j.nativeblas">Nd4jCpu.reduce_norm1</a>
<div class="block">This op calculates norm1 of elements along given dimensions

 input array:
    x: tensor to calculate norm1 for

 float arguments:
   keepDims: if non zero, then keep reduced dimensions with length = 1, default value is zero

 int arguments:
    list of integers - dimensions to calculate norm1 along, default corresponds to empty list in which case calculation is performed for all dimensions and scalar is returned

 output array:
    reduced tensor with calculated norm1</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_norm1_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_norm1_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_norm2.html#org.nd4j.nativeblas">Nd4jCpu.reduce_norm2</a>
<div class="block">This op calculates norm2 of elements along given dimensions

 input array:
    x: tensor to calculate norm2 for

 float arguments:
   keepDims: if non zero, then keep reduced dimensions with length = 1, default value is zero

 int arguments:
    list of integers - dimensions to calculate norm2 along, default corresponds to empty list in which case calculation is performed for all dimensions and scalar is returned

 output array:
    reduced tensor with calculated norm2</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_norm2_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_norm2_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_prod.html#org.nd4j.nativeblas">Nd4jCpu.reduce_prod</a>
<div class="block">reduction_prod - tf.reduction_prod operation

 input params:
    0 - NDArray

 T_ARG param (optional):
 0 - keep_dims !</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_prod_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_prod_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_sqnorm.html#org.nd4j.nativeblas">Nd4jCpu.reduce_sqnorm</a>
<div class="block">This op calculates squared norm of elements along given dimensions

 input array:
    x: tensor to calculate squared norm for

 float arguments:
   keepDims: if non zero, then keep reduced dimensions with length = 1, default value is zero

 int arguments:
    list of integers - dimensions to calculate squared norm along, default corresponds to empty list in which case calculation is performed for all dimensions and scalar is returned

 output array:
    reduced tensor with calculated norm</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_sqnorm_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_sqnorm_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_stdev.html#org.nd4j.nativeblas">Nd4jCpu.reduce_stdev</a>
<div class="block">This op calculates sample standard deviation of elements along given dimensions

 input array:
    x: tensor to calculate mean for

 float arguments:
   keepDims: if non zero, then keep reduced dimensions with length = 1, default value is zero
   biasCorrected - if non zero, then bias correction will be applied, default value is zero

 int arguments:
    list of integers - dimensions to calculate mean along, default corresponds to empty list in which case calculation is performed for all dimensions and scalar is returned

 output array:
    reduced tensor with calculated means</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_stdev_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_stdev_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_sum.html#org.nd4j.nativeblas">Nd4jCpu.reduce_sum</a>
<div class="block">reduction_sum - tf.reduction_sum operation

 input params:
    0 - NDArray

 T_ARG param (optional):
 0 - keep_dims !</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_sum_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_sum_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_variance.html#org.nd4j.nativeblas">Nd4jCpu.reduce_variance</a>
<div class="block">This op calculates sample variance of elements along given dimensions

 input array:
    x: tensor to calculate mean for

 float arguments:
   keepDims: if non zero, then keep reduced dimensions with length = 1, default value is zero
   biasCorrected -  if non zero, then bias correction will be applied, default value is zero

 int arguments:
    list of integers - dimensions to calculate mean along, default corresponds to empty list in which case calculation is performed for all dimensions and scalar is returned

 output array:
    reduced tensor with calculated means</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reduce_variance_bp.html#org.nd4j.nativeblas">Nd4jCpu.reduce_variance_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.relu.html#org.nd4j.nativeblas">Nd4jCpu.relu</a>
<div class="block">This is RELU activation function implementation</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.relu_bp.html#org.nd4j.nativeblas">Nd4jCpu.relu_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.relu_layer.html#org.nd4j.nativeblas">Nd4jCpu.relu_layer</a>
<div class="block">relu_layer = relu(x*w + b)</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.relu6.html#org.nd4j.nativeblas">Nd4jCpu.relu6</a>
<div class="block">This is RELU6 activation function implementation</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.relu6_bp.html#org.nd4j.nativeblas">Nd4jCpu.relu6_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.repeat.html#org.nd4j.nativeblas">Nd4jCpu.repeat</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reshape.html#org.nd4j.nativeblas">Nd4jCpu.reshape</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reshapeas.html#org.nd4j.nativeblas">Nd4jCpu.reshapeas</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.resize_bicubic.html#org.nd4j.nativeblas">Nd4jCpu.resize_bicubic</a>
<div class="block">This op make bicubic interpolated resize for given tensor

 input array:
    0 - 4D-Tensor with shape (batch, sizeX, sizeY, channels)
    1 - 1D-Tensor with 2 values (newWidth, newHeight)

 output array:
   the 4D-Tensor with resized image (shape is {batch, newWidth, newHeight, channels})</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.resize_bilinear.html#org.nd4j.nativeblas">Nd4jCpu.resize_bilinear</a>
<div class="block">This op make bilinear interpolated resize for given tensor

 input array:
    0 - 4D-Tensor with shape (batch, sizeX, sizeY, channels)
    1 - 1D-Tensor with 2 values (newWidth, newHeight) (optional)

 int arguments: (optional)
   0 - new width
   1 - new height

 output array:
   the 4D-Tensor with calculated backproped dots

 CAUTION: either size tensor or a pair of int params should be provided.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.resize_nearest_neighbor.html#org.nd4j.nativeblas">Nd4jCpu.resize_nearest_neighbor</a>
<div class="block">This op make nearest neighbor interpolated resize for given tensor

 input array:
    0 - 4D-Tensor with shape (batch, sizeX, sizeY, channels)
    1 - 1D-Tensor with 2 values (newWidth, newHeight) (optional)

 int arguments: (optional)
   0 - new width
   1 - new height

 output array:
   the 4D-Tensor with resized image (shape is {batch, newWidth, newHeight, channels})

 CAUTION: either size tensor or a pair of int params should be provided.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ResultSet.html#org.nd4j.nativeblas">Nd4jCpu.ResultSet</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Return.html#org.nd4j.nativeblas">Nd4jCpu.Return</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reverse.html#org.nd4j.nativeblas">Nd4jCpu.reverse</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reverse_bp.html#org.nd4j.nativeblas">Nd4jCpu.reverse_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reverse_sequence.html#org.nd4j.nativeblas">Nd4jCpu.reverse_sequence</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reversedivide.html#org.nd4j.nativeblas">Nd4jCpu.reversedivide</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reversedivide_bp.html#org.nd4j.nativeblas">Nd4jCpu.reversedivide_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reversemod.html#org.nd4j.nativeblas">Nd4jCpu.reversemod</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reversemod_bp.html#org.nd4j.nativeblas">Nd4jCpu.reversemod_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reversesubtract.html#org.nd4j.nativeblas">Nd4jCpu.reversesubtract</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.reversesubtract_bp.html#org.nd4j.nativeblas">Nd4jCpu.reversesubtract_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.rint.html#org.nd4j.nativeblas">Nd4jCpu.rint</a>
<div class="block">This operation applies element-wise rint (round to integral value) operation</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.roll.html#org.nd4j.nativeblas">Nd4jCpu.roll</a>
<div class="block">roll - op porting from numpy (https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.roll.html)

 input params:
    0 - NDArray

 int params:
    0 - shift
    1 - axe 1
    2 - axe 2
    ...</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.rshift_bits.html#org.nd4j.nativeblas">Nd4jCpu.rshift_bits</a>
<div class="block">This operation shift individual bits of each element in array to the right: >>

 PLEASE NOTE: This operation is applicable only to integer data types

 \tparam T</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_add.html#org.nd4j.nativeblas">Nd4jCpu.scatter_add</a>
<div class="block">This operation applies Add operation to specific inputs wrt indices
 Expected arguments:
 input: array to be updated
 indices: array containing indexes for first dimension of input
 updates: array containing elements to be interfered with input</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_div.html#org.nd4j.nativeblas">Nd4jCpu.scatter_div</a>
<div class="block">This operation applies Divide operation to specific inputs wrt indices
 Expected arguments:
 input: array to be updated
 indices: array containing indexes for first dimension of input
 updates: array containing elements to be interfered with input</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_list.html#org.nd4j.nativeblas">Nd4jCpu.scatter_list</a>
<div class="block">This operation unpacks given NDArray into specified NDArrayList wrt specified indices</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_max.html#org.nd4j.nativeblas">Nd4jCpu.scatter_max</a>
<div class="block">This operation applies Max operation to specific inputs through given indices
 Expected arguments:
 input: array to be updated
 indices: array containing indexes for first dimension of input
 updates: array containing elements to be interfered with input</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_min.html#org.nd4j.nativeblas">Nd4jCpu.scatter_min</a>
<div class="block">This operation applies Min operation to specific inputs through given indices
 Expected arguments:
 input: array to be updated
 indices: array containing indexes for first dimension of input
 updates: array containing elements to be interfered with input</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_mul.html#org.nd4j.nativeblas">Nd4jCpu.scatter_mul</a>
<div class="block">This operation applies Multiply operation to specific inputs wrt indices
 Expected arguments:
 input: array to be updated
 indices: array containing indexes for first dimension of input
 updates: array containing elements to be interfered with input</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_nd.html#org.nd4j.nativeblas">Nd4jCpu.scatter_nd</a>
<div class="block">This operation scatter "updates" elements into new output array according to given "indices"
 Expected arguments:
 indices: array containing elements/slices indexes of output array to put "updates" elements into, the rest output elements will be zeros
 updates: array containing elements to be inserted into output array
 shape: contains shape of output array</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_nd_add.html#org.nd4j.nativeblas">Nd4jCpu.scatter_nd_add</a>
<div class="block">This operation adds "updates" elements to input array along given "indices"
 Expected arguments:
 input: array to be updated
 indices: array containing elements/slices indexes of input array to add "updates" elements to
 updates: array containing elements to be interfered with input</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_nd_sub.html#org.nd4j.nativeblas">Nd4jCpu.scatter_nd_sub</a>
<div class="block">This operation subtract "updates" elements from input array along given "indices"
 Expected arguments:
 input: array to be updated
 indices: array containing elements/slices indexes of input array to subtract "updates" elements from
 updates: array containing elements to be interfered with input</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_nd_update.html#org.nd4j.nativeblas">Nd4jCpu.scatter_nd_update</a>
<div class="block">This operation scatter "updates" elements into input array along given "indices"
 Expected arguments:
 input: array to be updated
 indices: array containing elements/slices indexes of input array to put "updates" elements into
 updates: array containing elements to be inserted into input array</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_sub.html#org.nd4j.nativeblas">Nd4jCpu.scatter_sub</a>
<div class="block">This operation applies Subtract operation to specific inputs wrt indices
 Expected arguments:
 input: array to be updated
 indices: array containing indexes for first dimension of input
 updates: array containing elements to be interfered with input</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_upd.html#org.nd4j.nativeblas">Nd4jCpu.scatter_upd</a>
<div class="block">This operation applies Assign operation to specific inputs wrt indices
 Expected arguments:
 input: array to be updated
 indices: array containing indexes for first dimension of input
 updates: array containing elements to be interfered with input</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.scatter_update.html#org.nd4j.nativeblas">Nd4jCpu.scatter_update</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sconv2d.html#org.nd4j.nativeblas">Nd4jCpu.sconv2d</a>
<div class="block">Depthwise convolution2d op:
 Expected inputs:
 x: 4D array, NCHW format
 weightsDepth: 4D array,
 weightsPointwise: optional, 4D array
 bias: optional, vector</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sconv2d_bp.html#org.nd4j.nativeblas">Nd4jCpu.sconv2d_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Scope.html#org.nd4j.nativeblas">Nd4jCpu.Scope</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_max.html#org.nd4j.nativeblas">Nd4jCpu.segment_max</a>
<div class="block">segment_max op. - make a tensor filled by max values according to index tensor given.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_max_bp.html#org.nd4j.nativeblas">Nd4jCpu.segment_max_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_mean.html#org.nd4j.nativeblas">Nd4jCpu.segment_mean</a>
<div class="block">segment_mean op. - make a tensor filled by average of values according to index tensor given.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_mean_bp.html#org.nd4j.nativeblas">Nd4jCpu.segment_mean_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_min.html#org.nd4j.nativeblas">Nd4jCpu.segment_min</a>
<div class="block">segment_min op. - make a tensor filled by min values according to index tensor given.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_min_bp.html#org.nd4j.nativeblas">Nd4jCpu.segment_min_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_prod.html#org.nd4j.nativeblas">Nd4jCpu.segment_prod</a>
<div class="block">segment_prod op. - make a tensor filled by product of values according to index tensor given.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_prod_bp.html#org.nd4j.nativeblas">Nd4jCpu.segment_prod_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_sum.html#org.nd4j.nativeblas">Nd4jCpu.segment_sum</a>
<div class="block">segment_sum op. - make a tensor filled by sum of values according to index tensor given.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.segment_sum_bp.html#org.nd4j.nativeblas">Nd4jCpu.segment_sum_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.select.html#org.nd4j.nativeblas">Nd4jCpu.select</a>
<div class="block">This op takes 2 n-dimensional arrays as input, and return
 array of the same shape, with elements, either from x or y, depending on the condition.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.selu.html#org.nd4j.nativeblas">Nd4jCpu.selu</a>
<div class="block">This is SELU activation function implementation</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.selu_bp.html#org.nd4j.nativeblas">Nd4jCpu.selu_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sequence_mask.html#org.nd4j.nativeblas">Nd4jCpu.sequence_mask</a>
<div class="block">sequence_mask op. - make mask for given tensor filled by (j > x[i_1, i_2,...</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.set_seed.html#org.nd4j.nativeblas">Nd4jCpu.set_seed</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.shape_of.html#org.nd4j.nativeblas">Nd4jCpu.shape_of</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ShapeDescriptor.html#org.nd4j.nativeblas">Nd4jCpu.ShapeDescriptor</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ShapeInformation.html#org.nd4j.nativeblas">Nd4jCpu.ShapeInformation</a>
<div class="block">Shape information approximating
 the information on an ndarray</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.ShapeList.html#org.nd4j.nativeblas">Nd4jCpu.ShapeList</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.shapes_of.html#org.nd4j.nativeblas">Nd4jCpu.shapes_of</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.shift_bits.html#org.nd4j.nativeblas">Nd4jCpu.shift_bits</a>
<div class="block">This operation shift individual bits of each element in array to the left: <<

 PLEASE NOTE: This operation is applicable only to integer data types

 \tparam T</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sigm_cross_entropy_loss.html#org.nd4j.nativeblas">Nd4jCpu.sigm_cross_entropy_loss</a>
<div class="block">Implementation of sigmoid cross-entropy loss function max(logits, 0.) - logits * labels + log(1. + exp(-abs(logits))); 
 
 Input arrays: 
    0: logits - logits, type float
    1: weights - is used for weighting (multiplying) of loss values, type float.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sigm_cross_entropy_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.sigm_cross_entropy_loss_grad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sigmoid.html#org.nd4j.nativeblas">Nd4jCpu.sigmoid</a>
<div class="block">This is Sigmoid activation function implementation
 Math is: 1 / 1 + exp(-x)</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sigmoid_bp.html#org.nd4j.nativeblas">Nd4jCpu.sigmoid_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.size.html#org.nd4j.nativeblas">Nd4jCpu.size</a>
<div class="block">This operation returns length of input array
 Expected arguments:
 input: N-dimensional array

 TODO: make this operation reduction, to allow TAD -> size</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.size_at.html#org.nd4j.nativeblas">Nd4jCpu.size_at</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.size_list.html#org.nd4j.nativeblas">Nd4jCpu.size_list</a>
<div class="block">This operations returns scalar, with number of existing arrays within given NDArrayList
 Expected arguments:
 x: list</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.slice.html#org.nd4j.nativeblas">Nd4jCpu.slice</a>
<div class="block">This operation extracts a slice from a tensor.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.slice_bp.html#org.nd4j.nativeblas">Nd4jCpu.slice_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softmax.html#org.nd4j.nativeblas">Nd4jCpu.softmax</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softmax_bp.html#org.nd4j.nativeblas">Nd4jCpu.softmax_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softmax_cross_entropy_loss.html#org.nd4j.nativeblas">Nd4jCpu.softmax_cross_entropy_loss</a>
<div class="block">Implementation of softmax cross-entropy loss function max(logits, 0.) - logits * labels + log(1. + exp(-abs(logits))); 
 
 Input arrays: 
    0: logits - logits, type float
    1: weights - is used for weighting (multiplying) of loss values, type float.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softmax_cross_entropy_loss_grad.html#org.nd4j.nativeblas">Nd4jCpu.softmax_cross_entropy_loss_grad</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softmax_cross_entropy_loss_with_logits.html#org.nd4j.nativeblas">Nd4jCpu.softmax_cross_entropy_loss_with_logits</a>
<div class="block">Implementation of softmax cross-entropy loss function 
 
 Input arrays: 
    0: logits - logits, type float
    1: labels - ground truth vales, expected to be 0. or 1., type float.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softmax_cross_entropy_loss_with_logits_grad.html#org.nd4j.nativeblas">Nd4jCpu.softmax_cross_entropy_loss_with_logits_grad</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softplus.html#org.nd4j.nativeblas">Nd4jCpu.softplus</a>
<div class="block">This is Softplus activation function implementation
 Math is: log(1 + exp(x))</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softplus_bp.html#org.nd4j.nativeblas">Nd4jCpu.softplus_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softsign.html#org.nd4j.nativeblas">Nd4jCpu.softsign</a>
<div class="block">This is Softsign activation function implementation
 Math is: x / 1 + abs(x)</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.softsign_bp.html#org.nd4j.nativeblas">Nd4jCpu.softsign_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.space_to_batch.html#org.nd4j.nativeblas">Nd4jCpu.space_to_batch</a>
<div class="block">Zero-pads and then rearranges (permutes) blocks of spatial data into batch.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.space_to_batch_nd.html#org.nd4j.nativeblas">Nd4jCpu.space_to_batch_nd</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.space_to_depth.html#org.nd4j.nativeblas">Nd4jCpu.space_to_depth</a>
<div class="block">This operation rearranges blocks of spatial data, into depth.This op output is a copy of the input tensor
 where values from the height and width dimensions are moved to the depth dimension.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sparse_softmax_cross_entropy_loss_with_logits.html#org.nd4j.nativeblas">Nd4jCpu.sparse_softmax_cross_entropy_loss_with_logits</a>
<div class="block">Implementation of sparse softmax cross-entropy loss function
 
 Input arrays:        
    0: labels - ground truth vales, expected to be within range [0, num_classes), type float.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sparse_softmax_cross_entropy_loss_with_logits_grad.html#org.nd4j.nativeblas">Nd4jCpu.sparse_softmax_cross_entropy_loss_with_logits_grad</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.split.html#org.nd4j.nativeblas">Nd4jCpu.split</a>
<div class="block">This operation splits given NDArray into chunks of specific size, along given dimension
 0 - input array
 1 - optional axis

 Integer arguments:
 0 - number of splits
 1 - optional axis</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.split_list.html#org.nd4j.nativeblas">Nd4jCpu.split_list</a>
<div class="block">This operation splits given NDArray into chunks, and stores them into given NDArrayList wert sizes
 Expected arguments:
 list: optional, NDArrayList. if not available - new NDArrayList will be created
 array: array to be split
 sizes: vector with sizes for each chunk</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.split_v.html#org.nd4j.nativeblas">Nd4jCpu.split_v</a>
<div class="block">This operation splits given NDArray into chunks of specific size, along given dimension
 Input arrays:
 0 - input array
 1 - array of sizes
 2 - optional axis

 Integer arguments:
 0 - optional axis</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.square.html#org.nd4j.nativeblas">Nd4jCpu.square</a>
<div class="block">This operation applies element-wise pow(x, 2) to the given input
 Expected arguments:
 input: N-Dimensional array</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.squaredsubtract.html#org.nd4j.nativeblas">Nd4jCpu.squaredsubtract</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.squaredsubtract_bp.html#org.nd4j.nativeblas">Nd4jCpu.squaredsubtract_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.squeeze.html#org.nd4j.nativeblas">Nd4jCpu.squeeze</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sru.html#org.nd4j.nativeblas">Nd4jCpu.sru</a>
<div class="block">Implementation of operation for Simple Recurrent Unit: "Training RNNs as Fast as CNNs" Tao Lei, Yu Zhang, Yoav Artzi

 Input arrays:
    0: input 3d tensor with shape [bS x K x N], N - number of time steps, bS - batch size, K - number of features
    1: 2d tensor of weights [3K x K]
    2: row of biases with twice length [1 x 2K]
    3: 2d tensor of previous cell state [bS x K]
    4: optional, 2d tensor of dropout mask [bS x K]

 Output arrays:
    0: 3d tensor of cell output [bS x K x N]
    1: 3d tensor of cell state [bS x K x N]</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sru_bi.html#org.nd4j.nativeblas">Nd4jCpu.sru_bi</a>
<div class="block">Implementation of operation for Simple Recurrent Unit (bidirectional case): "Training RNNs as Fast as CNNs" Tao Lei, Yu Zhang, Yoav Artzi

 Input arrays:
    0: input 3d tensor with shape [N x bS x 2K], N - number of time steps, bS - batch size, K - number of features
    1: 2d tensor of weights [2K x 6K]
    2: row of biases with twice length [1 x 4K]
    3: 2d tensor of previous cell state [bS x 2K]
    4: optional, 2d tensor of dropout mask [bS x 2K]

 Output arrays:
    0: 3d tensor of cell output [N x bS x 2K]
    1: 3d tensor of cell state [N x bS x 2K]</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sru_bi_bp.html#org.nd4j.nativeblas">Nd4jCpu.sru_bi_bp</a>
<div class="block">Implementation of operation for back propagation in Simple Recurrent Unit (bidirectional case): "Training RNNs as Fast as CNNs" Tao Lei, Yu Zhang, Yoav Artzi

 Input arrays:
    0: input 3d tensor with shape [N x bS x 2K], N - number of time steps, bS - batch size, K - number of features
    1: 2d tensor of weights [2K x 6K]
    2: row of biases with twice length [1 x 4K]
    3: 2d tensor of previous cell state [bS x 2K]
    4: 3d tensor of cell state [N x bS x 2K]
    5: 2d tensor of cell state gradients [bS x 2K]
    6: 3d tensor of state output gradients [N x bS x 2K]
    7: optional, 2d tensor of dropout mask [bS x 2K]

 Output arrays:
    0: 3d tensor of input gradients [N x bS x 2K]
    1: 3d tensor of weights gradients [N x 2K x 6K]
    2: 2d, row of biases gradients [1 x 4K]
    3: 2d, tensor of state gradients [bS x 2K]</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sru_bp.html#org.nd4j.nativeblas">Nd4jCpu.sru_bp</a>
<div class="block">Implementation of operation for back propagation in Simple Recurrent Unit: "Training RNNs as Fast as CNNs" Tao Lei, Yu Zhang, Yoav Artzi

 Input arrays:
    0: input 3d tensor with shape [bS x K x N], N - number of time steps, bS - batch size, K - number of features
    1: 2d tensor of weights [3K x K]
    2: row of biases with twice length [1 x 2K]
    3: 2d tensor of previous cell state [bS x K]
    4: 3d tensor of cell state [bS x K x N]
    5: 2d tensor of cell state gradients [bS x K]
    6: 3d tensor of state output gradients [bS x K x N]
    7: optional, 2d tensor of dropout mask [bS x K]

 Output arrays:
    0: 3d tensor of input gradients [bS x K x N]
    1: 3d tensor of weights gradients [bS x 3K x K]
    2: 2d, row of biases gradients [1 x 2K]
    3: 2d, tensor of state gradients [bS x K]</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sruCell.html#org.nd4j.nativeblas">Nd4jCpu.sruCell</a>
<div class="block">Implementation of operations for Simple Recurrent Unit cell: "Training RNNs as Fast as CNNs" Tao Lei, Yu Zhang, Yoav Artzi

 Input arrays:
    0: input with shape [batchSize x inSize], batchSize - batch size, inSize - number of features
    1: previous cell state [batchSize x inSize], that is at previous time step t-1
    2: weights [inSize x 3*inSize]
    3: biases [1 x 2*inSize]

 Output arrays:
    0: current cell output [batchSize x inSize], that is at current time step t
    1: current cell state  [batchSize x inSize], that is at current time step t</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.stack.html#org.nd4j.nativeblas">Nd4jCpu.stack</a>
<div class="block">This operation stacks a list of rank tensors into one rank-(R+1) tensor.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.stack_list.html#org.nd4j.nativeblas">Nd4jCpu.stack_list</a>
<div class="block">This operation concatenates given NDArrayList, and returns NDArray as result</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.standardize.html#org.nd4j.nativeblas">Nd4jCpu.standardize</a>
<div class="block">standardizes input array to be zero mean unit variance along the given axis</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.standardize_bp.html#org.nd4j.nativeblas">Nd4jCpu.standardize_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Stash.html#org.nd4j.nativeblas">Nd4jCpu.Stash</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.static_bidirectional_rnn.html#org.nd4j.nativeblas">Nd4jCpu.static_bidirectional_rnn</a>
<div class="block">Implementation of operation "static RNN time sequences" with peep hole connections:

 Input arrays:
    0: input with shape [time x batchSize x inSize], time - number of time steps, batchSize - batch size, inSize - number of features
    1: input-to-hidden  weights for forward RNN, [inSize   x numUnitsFW]
    2: hidden-to-hidden weights for forward RNN, [numUnitsFW x numUnitsFW]
    3: biases for forward RNN, [2*numUnitsFW]
    4: input-to-hidden  weights for backward RNN, [inSize   x numUnitsBW]
    5: hidden-to-hidden weights for backward RNN, [numUnitsBW x numUnitsBW]
    6: biases for backward RNN, [2*numUnitsBW]
    7: (optional) initial cell output for forward RNN [batchSize x numUnitsFW], that is at time step = 0
    8: (optional) initial cell output for backward RNN [batchSize x numUnitsBW], that is at time step = 0
    9: (optional) vector with shape [batchSize] containing integer values within [0,time), each element of this vector set max time step per each input in batch, this provides no calculations for time >= maxTimeStep

 Output arrays:
    0: cell outputs [time x batchSize x (numUnitsFW + numUnitsBW)]
    1: cell final non-zero output for forward RNN  [batchSize x numUnitsFW]
    2: cell final non-zero output for backward RNN [batchSize x numUnitsBW]</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.static_rnn.html#org.nd4j.nativeblas">Nd4jCpu.static_rnn</a>
<div class="block">Implementation of operation "static RNN time sequences" with peep hole connections:

 Input arrays:
    0: input with shape [time x batchSize x inSize], time - number of time steps, batchSize - batch size, inSize - number of features
    1: input-to-hidden  weights, [inSize   x numUnits]
    2: hidden-to-hidden weights, [numUnits x numUnits]
    3: biases, [2*numUnits]
    4: (optional) initial cell output [batchSize x numUnits], that is at time step = 0
    5: (optional) vector with shape [batchSize] containing integer values within [0,time), each element of this vector set max time step per each input in batch, this provides no calculations for time >= maxTimeStep

 Output arrays:
    0: cell outputs [time x batchSize x numUnits]
    1: cell final non-zero output [batchSize x numUnits]</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.stop_gradient.html#org.nd4j.nativeblas">Nd4jCpu.stop_gradient</a>
<div class="block">This operation is missed due it simplicy.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.strided_slice.html#org.nd4j.nativeblas">Nd4jCpu.strided_slice</a>
<div class="block">This operation extracts a strided (optionally) slice from a tensor,</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.strided_slice_bp.html#org.nd4j.nativeblas">Nd4jCpu.strided_slice_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.subtract.html#org.nd4j.nativeblas">Nd4jCpu.subtract</a>
<div class="block">This is one of auto-broadcastable operations.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.subtract_bp.html#org.nd4j.nativeblas">Nd4jCpu.subtract_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.sufficient_statistics.html#org.nd4j.nativeblas">Nd4jCpu.sufficient_statistics</a>
<div class="block">sufficient_statistics operation return calculated mean and variation with data count.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.svd.html#org.nd4j.nativeblas">Nd4jCpu.svd</a>
<div class="block">performs singular value decomposition (SVD) of one or more matrices, evaluates the SVD of each inner-most 2D matrix in input array:
 x[..., :, :] = u[..., :, :] * s[...,:] * transpose(v[..., :, :]) 

 Input array:
 x[..., Rows, Cols], the necessary condition is: rank of x >= 2
 
 Outputs arrays:
 s[..., diagSize] - array with singular values which are stored in decreasing order, diagSize is smaller among Rows and Cols
 u[..., Rows, Rows] if IArgs[1] is true, else u[..., Rows, diagSize] - array with right singular vectors
 v[..., Cols, Cols] if IArgs[1] is true, else v[..., Cols, diagSize] - array with left singular vectors
 
 Integer arguments:
 IArgs[0] - bool, whether to calculate u and v, s is calculated in any case
 IArgs[1] - bool, whether to calculate full-sized u and v
 IArgs[2] - the number of cols or rows which determines what algorithm to use.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Switch.html#org.nd4j.nativeblas">Nd4jCpu.Switch</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.TadDescriptor.html#org.nd4j.nativeblas">Nd4jCpu.TadDescriptor</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.TadPack.html#org.nd4j.nativeblas">Nd4jCpu.TadPack</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tanh.html#org.nd4j.nativeblas">Nd4jCpu.tanh</a>
<div class="block">This is Tanh activation function implementation</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tanh_bp.html#org.nd4j.nativeblas">Nd4jCpu.tanh_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tear.html#org.nd4j.nativeblas">Nd4jCpu.tear</a>
<div class="block">This operation splits input NDArray into multiple TADs along given dimensions
 Expected arguments:
 input: N-dimensional array

 Int args:
 0..: TAD axis</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tensormmul.html#org.nd4j.nativeblas">Nd4jCpu.tensormmul</a>
<div class="block">tensorMmul/tensorDot operation
 takes 2 ndarrays, and 2 sets of axes

 Integer argumens map:
 IArgs[0] - number of axes along for first array
 IArgs[1]... axes values for first array
 IArgs[] - number of axes along for second array
 IArgs[1]... axes values for second array</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.test_output_reshape.html#org.nd4j.nativeblas">Nd4jCpu.test_output_reshape</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.test_scalar.html#org.nd4j.nativeblas">Nd4jCpu.test_scalar</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.testcustom.html#org.nd4j.nativeblas">Nd4jCpu.testcustom</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.testop2i2o.html#org.nd4j.nativeblas">Nd4jCpu.testop2i2o</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.testreduction.html#org.nd4j.nativeblas">Nd4jCpu.testreduction</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tf_atan2.html#org.nd4j.nativeblas">Nd4jCpu.tf_atan2</a>
<div class="block">Special atan2 op impl for TF's args order
 \tparam T</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.thresholdedrelu.html#org.nd4j.nativeblas">Nd4jCpu.thresholdedrelu</a>
<div class="block">Thresholded Rectified Linear Unit
 f(x) = x for x > theta, f(x) = 0 otherwise
 theta must be >= 0</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.thresholdedrelu_bp.html#org.nd4j.nativeblas">Nd4jCpu.thresholdedrelu_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tile.html#org.nd4j.nativeblas">Nd4jCpu.tile</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tile_bp.html#org.nd4j.nativeblas">Nd4jCpu.tile_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tile_to_shape.html#org.nd4j.nativeblas">Nd4jCpu.tile_to_shape</a>
<div class="block">This op boosts specified input up to specified shape

 \tparam T</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tile_to_shape_bp.html#org.nd4j.nativeblas">Nd4jCpu.tile_to_shape_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.to_double.html#org.nd4j.nativeblas">Nd4jCpu.to_double</a>
<div class="block">This operation casts elements of input array to double data type
 
 PLEASE NOTE: This op is disabled atm, and reserved for future releases.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.to_float16.html#org.nd4j.nativeblas">Nd4jCpu.to_float16</a>
<div class="block">This operation casts elements of input array to float16 data type
 
 PLEASE NOTE: This op is disabled atm, and reserved for future releases.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.to_float32.html#org.nd4j.nativeblas">Nd4jCpu.to_float32</a>
<div class="block">This operation casts elements of input array to float data type
 
 PLEASE NOTE: This op is disabled atm, and reserved for future releases.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.to_int32.html#org.nd4j.nativeblas">Nd4jCpu.to_int32</a>
<div class="block">This operation casts elements of input array to int32 data type
 
 PLEASE NOTE: This op is disabled atm, and reserved for future releases.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.to_int64.html#org.nd4j.nativeblas">Nd4jCpu.to_int64</a>
<div class="block">This operation casts elements of input array to int64 (aka long long) data type
 
 PLEASE NOTE: This op is disabled atm, and reserved for future releases.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.to_uint32.html#org.nd4j.nativeblas">Nd4jCpu.to_uint32</a>
<div class="block">This operation casts elements of input array to unsinged int32 data type
 
 PLEASE NOTE: This op is disabled atm, and reserved for future releases.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.to_uint64.html#org.nd4j.nativeblas">Nd4jCpu.to_uint64</a>
<div class="block">This operation casts elements of input array to unsigned int64 (aka unsigned long long) data type
 
 PLEASE NOTE: This op is disabled atm, and reserved for future releases.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.toggle_bits.html#org.nd4j.nativeblas">Nd4jCpu.toggle_bits</a>
<div class="block">This operation toggles individual bits of each element in array
 
 PLEASE NOTE: This operation is possible only on integer data types
 
 \tparam T</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.top_k.html#org.nd4j.nativeblas">Nd4jCpu.top_k</a>
<div class="block">top_k operation returns a vector of k top values for
  given NDArray as tensor with default boolean (true)
  as sort for result index array
  will be sorted by the values in descending order.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.trace.html#org.nd4j.nativeblas">Nd4jCpu.trace</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.transpose.html#org.nd4j.nativeblas">Nd4jCpu.transpose</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.tri.html#org.nd4j.nativeblas">Nd4jCpu.tri</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.triu.html#org.nd4j.nativeblas">Nd4jCpu.triu</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.triu_bp.html#org.nd4j.nativeblas">Nd4jCpu.triu_bp</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.truncatediv.html#org.nd4j.nativeblas">Nd4jCpu.truncatediv</a>
<div class="block">\tparam T</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unique.html#org.nd4j.nativeblas">Nd4jCpu.unique</a>
<div class="block">This operation returns unique elements from input array as vector, and their original indices in input array
 Expected input:
 input: N-dimensional array</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unique_with_counts.html#org.nd4j.nativeblas">Nd4jCpu.unique_with_counts</a>
<div class="block">This operation returns 3 1D arrays for given 1D array with unique element count and indexes
 input:
     0 - 1D array

 output:
     0 - 1D array with unique values
     1 - 1D array with ids for values in array above
     2 - 1D array with counts for values in array above</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_max.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_max</a>
<div class="block">unsorted_segment_max op. - make a tensor filled by max values according to index tensor given.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_max_bp.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_max_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_mean.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_mean</a>
<div class="block">unsorted_segment_mean op. - make a tensor filled by average of values according to index tensor given.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_mean_bp.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_mean_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_min.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_min</a>
<div class="block">unsorted_segment_min op. - make a tensor filled by min values according to index tensor given.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_min_bp.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_min_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_prod.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_prod</a>
<div class="block">unsorted_segment_prod op. - make a tensor filled by product of values according to index tensor given.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_prod_bp.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_prod_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_sqrt_n.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_sqrt_n</a>
<div class="block">unsorted_segment_sqrt_n op. - computes the sum along segments of a tensor divided by the sqrt(N).</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_sqrt_n_bp.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_sqrt_n_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_sum.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_sum</a>
<div class="block">unsorted_segment_sum op. - make a tensor filled by sum of values according to index tensor given.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unsorted_segment_sum_bp.html#org.nd4j.nativeblas">Nd4jCpu.unsorted_segment_sum_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unstack.html#org.nd4j.nativeblas">Nd4jCpu.unstack</a>
<div class="block">This op does the same as tear, just uses different input format:
 \tparam T</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.unstack_list.html#org.nd4j.nativeblas">Nd4jCpu.unstack_list</a>
<div class="block">This operation unstacks given NDArray into NDArrayList by the first dimension</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.upsampling2d.html#org.nd4j.nativeblas">Nd4jCpu.upsampling2d</a>
<div class="block">Expected input: 4D array

 IntArgs:
 0: scale factor for rows (height)
 1: scale factor for columns (width)
 2: data format: 0 NHWC (default), 1 NCHW</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.upsampling2d_bp.html#org.nd4j.nativeblas">Nd4jCpu.upsampling2d_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.upsampling3d.html#org.nd4j.nativeblas">Nd4jCpu.upsampling3d</a>
<div class="block">Expected input: 4D array

 IntArgs:
 0: scale factor for depth
 1: scale factor for rows (height)
 2: scale factor for columns (width)
 3: data format: 0 NDHWC (default), 1 NCDHW</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.upsampling3d_bp.html#org.nd4j.nativeblas">Nd4jCpu.upsampling3d_bp</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.utf8string.html#org.nd4j.nativeblas">Nd4jCpu.utf8string</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Variable.html#org.nd4j.nativeblas">Nd4jCpu.Variable</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.VariableSpace.html#org.nd4j.nativeblas">Nd4jCpu.VariableSpace</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.weighted_cross_entropy_with_logits.html#org.nd4j.nativeblas">Nd4jCpu.weighted_cross_entropy_with_logits</a>
<div class="block">This op calculates weighted logarithmic loss of input
 Input arguments
  0 - target
  1 - input
  2 - weights (scalar or vector with same as last dimension)

  return value - a tensor with the same shape as target or input</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Where.html#org.nd4j.nativeblas">Nd4jCpu.Where</a>
<div class="block">This op takes 2 n-dimensional arrays as input, and return 
 array of the same shape, with elements, either from x or y, depending on the condition.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.where_np.html#org.nd4j.nativeblas">Nd4jCpu.where_np</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.While.html#org.nd4j.nativeblas">Nd4jCpu.While</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.Workspace.html#org.nd4j.nativeblas">Nd4jCpu.Workspace</a>
<div class="block">Copyright (c) 2015-2018 Skymind, Inc.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.write_list.html#org.nd4j.nativeblas">Nd4jCpu.write_list</a>
<div class="block">This operations puts given NDArray into (optionally) given NDArrayList.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.xw_plus_b.html#org.nd4j.nativeblas">Nd4jCpu.xw_plus_b</a>
<div class="block">xw_plus_b op.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.zero_fraction.html#org.nd4j.nativeblas">Nd4jCpu.zero_fraction</a>
<div class="block">zero_fraction op.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.zeros_as.html#org.nd4j.nativeblas">Nd4jCpu.zeros_as</a>
<div class="block">This operation takes input's shape, and returns new NDArray filled with zeros
 Expected arguments:
 input: N-dimensional array</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpu.zeta.html#org.nd4j.nativeblas">Nd4jCpu.zeta</a>
<div class="block">This op calculates Hurwitz zeta function zeta(x, q) = sum_{n=0}^{inf} (q + n)^{-x}
 Implementation is based on Euler-Maclaurin summation formula

   Input arrays:
   x: define power {-x}, must be > 1, type float.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpuHelper.html#org.nd4j.nativeblas">Nd4jCpuHelper</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/Nd4jCpuPresets.html#org.nd4j.nativeblas">Nd4jCpuPresets</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/OpaqueConstantDataBuffer.html#org.nd4j.nativeblas">OpaqueConstantDataBuffer</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/OpaqueContext.html#org.nd4j.nativeblas">OpaqueContext</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/OpaqueLaunchContext.html#org.nd4j.nativeblas">OpaqueLaunchContext</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/OpaqueRandomGenerator.html#org.nd4j.nativeblas">OpaqueRandomGenerator</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/OpaqueResultWrapper.html#org.nd4j.nativeblas">OpaqueResultWrapper</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/OpaqueShapeList.html#org.nd4j.nativeblas">OpaqueShapeList</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/OpaqueTadPack.html#org.nd4j.nativeblas">OpaqueTadPack</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/OpaqueVariable.html#org.nd4j.nativeblas">OpaqueVariable</a>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/OpaqueVariablesSet.html#org.nd4j.nativeblas">OpaqueVariablesSet</a>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/ResultWrapperAbstraction.html#org.nd4j.nativeblas">ResultWrapperAbstraction</a>&nbsp;</td>
</tr>
</tbody>
</table>
</li>
<li class="blockList"><a name="org.nd4j.rng">
<!--   -->
</a>
<table class="useSummary" border="0" cellpadding="3" cellspacing="0" summary="Use table, listing classes, and an explanation">
<caption><span>Classes in <a href="../../../org/nd4j/nativeblas/package-summary.html">org.nd4j.nativeblas</a> used by <a href="../../../org/nd4j/rng/package-summary.html">org.nd4j.rng</a></span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Class and Description</th>
</tr>
<tbody>
<tr class="altColor">
<td class="colOne"><a href="../../../org/nd4j/nativeblas/class-use/NativeOps.html#org.nd4j.rng">NativeOps</a>
<div class="block">Native interface for
 op execution on cpu</div>
</td>
</tr>
</tbody>
</table>
</li>
</ul>
</div>
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar.bottom">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.bottom" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.bottom.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li>Class</li>
<li class="navBarCell1Rev">Use</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../index-all.html">Index</a></li>
<li><a href="../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li>Prev</li>
<li>Next</li>
</ul>
<ul class="navList">
<li><a href="../../../index.html?org/nd4j/nativeblas/package-use.html" target="_top">Frames</a></li>
<li><a href="package-use.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<a name="skip.navbar.bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
<p class="legalCopy"><small>Copyright &#169; 2020. All rights reserved.</small></p>
</body>
</html>
